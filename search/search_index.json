{"config":{"lang":["en","fr"],"separator":"[\\s\\-\\.]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Introduction","text":""},{"location":"index.html#calcul-distribue-avec-spark","title":"Calcul distribu\u00e9 avec Spark","text":""},{"location":"index.html#objectifs","title":"Objectifs","text":"<ul> <li>Utiliser Spark sous Hadoop</li> <li>Cr\u00e9er des programmes avec l'API Spark</li> <li>Manipuler les diff\u00e9rentes structures de donn\u00e9es Spark : RDD, DataFrame</li> <li>Interroger les donn\u00e9es avec SparKSQL</li> <li>Traiter des flux de donn\u00e9es avec Spark Streaming</li> <li>Cr\u00e9er des pipeline ML</li> </ul> <p>Outils</p> <ul> <li>Image de machine virtuelle  avec les outils pr\u00e9-install\u00e9s. C'est une image cr\u00e9\u00e9e par Pierre Nezric \u00e0 laquelle sont ajout\u00e9s jupyter et mrjob. Elle contient les outils suivants :<ul> <li>Hadoop 2.7.3</li> <li>Spark 2.1.1</li> <li>Pig 0.15.0</li> <li>Hive 1.2.1</li> <li>HBase 1.1.9</li> <li>Cassandra </li> <li>Elasticsearch et Kibana</li> <li>Zookeeper 3.4.6</li> </ul> </li> </ul> <p>Sources et r\u00e9f\u00e9rences</p> <ul> <li>Documentation Spark</li> </ul>"},{"location":"index.html#presentation-de-spark","title":"Pr\u00e9sentation de Spark","text":"<p>Avec MapReduce, la sp\u00e9cification de l\u2019it\u00e9ration reste \u00e0 la charge du programmeur; il faut stocker le r\u00e9sultat d\u2019un premier job dans une collection interm\u00e9diaire et r\u00e9iterer le job en prenant la collection interm\u00e9diaire comme source. C\u2019est laborieux pour l\u2019implantation, et surtout tr\u00e8s peu efficace quand la collection interm\u00e9diaire est grande. Le processus de s\u00e9rialisation/d\u00e9s\u00e9rialisation sur disque propre \u00e0 la gestion de la reprise sur panne en MapReduce entra\u00eene des performances m\u00e9diocres.</p> <p>Dans Spark, la m\u00e9thode est tr\u00e8s diff\u00e9rente. Elle consiste \u00e0 placer ces jeux de donn\u00e9es en m\u00e9moire RAM et \u00e0 \u00e9viter la p\u00e9nalit\u00e9 des \u00e9critures sur le disque.</p> <p>Spark offre un \u00e9cosyst\u00e8me de librairies couvrant :</p> <ul> <li>L'apprentissage automatique : MlLib</li> <li>Donn\u00e9es structur\u00e9s et SQL : SparkSQL</li> <li>Flux de donn\u00e9es : Spark streaming et Spark Structured Streaming</li> <li>Graphes : GraphX</li> </ul> <p></p>"},{"location":"index.html#architecture-systeme","title":"Architecture syst\u00e8me","text":"<p>Le programmeur envoie au framework des Spark Applications, pour lesquelles Spark affecte des ressources (RAM, CPU) du cluster en vue de leur ex\u00e9cution. Un executor n\u2019est responsable que de 2 choses : ex\u00e9cuter le code qui lui est assign\u00e9 par le driver et lui rapporter l\u2019\u00e9tat d\u2019avancement de la t\u00e2che.</p> <p>Le driver est accessible programmatiquement par un point d\u2019entr\u00e9e appel\u00e9 SparkSession, que l\u2019on trouve derri\u00e8re une variable spark.</p> <p>Spark est \u00e9crit en Scala mais les programmes Spark peuvent \u00eatre \u00e9crits en Scala, Java, Python, SQL et R. Dans ce qui suit, nous utilisons Python \u00e0 travers la librairie Pyspark.</p>"},{"location":"index.html#rdds-dataframes-et-datasets","title":"RDDs, DataFrames et Datasets","text":"<p>La principale innovation apport\u00e9e par Spark est le concept de Resilient Distributed Dataset (RDD). Un RDD est une collection calcul\u00e9e \u00e0 partir d\u2019une source de donn\u00e9es (liste, fichier, ...) et plac\u00e9e en m\u00e9moire RAM. Spark conserve l\u2019historique des op\u00e9rations qui a permis de constituer un RDD, et la reprise sur panne s\u2019appuie essentiellement sur la pr\u00e9servation de cet historique afin de reconstituer le RDD en cas de panne. </p> <p>Chaque RDD \u00e9tant fragment\u00e9e, une panne affectant un fragment est r\u00e9par\u00e9e (par reconstitution de l\u2019historique) ind\u00e9pendamment des autres fragments, \u00e9vitant d\u2019avoir \u00e0 tout recalculer.</p> <p>Il est \u00e0 noter que ces RDD sont immuables et ne peuvent \u00eatre modifi\u00e9es apr\u00e8s leur cr\u00e9ation.</p> <p>Les DataFrames et Datasets se situe \u00e0 un niveau plus haut et reposent sur les RDDs. Il apporte une structure aux donn\u00e9es \u00e0 travers un sch\u00e9ma.</p>"},{"location":"index.html#pyspark","title":"Pyspark","text":"<p>PySpark est une biblioth\u00e8que Spark \u00e9crite en Python pour ex\u00e9cuter une application Python en utilisant les capacit\u00e9s d'Apache Spark, en utilisant PySpark, nous pouvons ex\u00e9cuter des applications en parall\u00e8le sur le cluster distribu\u00e9 (plusieurs n\u0153uds).</p> <p>Les modules et packages de pyspark sont :</p> <ul> <li>PySpark RDD (pyspark.RDD)</li> <li>PySpark DataFrame et SQL (pyspark.sql)</li> <li>PySpark Streaming (pyspark.streaming)</li> <li>PySpark MLib (pyspark.ml, pyspark.mllib)</li> <li>PySpark GraphFrames (GraphFrames)</li> <li>PySpark Resource (pyspark.resource)</li> </ul>"},{"location":"index.html#preparation-de-lenvironnement","title":"Pr\u00e9paration de l'environnement","text":"<ul> <li>En utilisant VirtualBox et \u00e0 partir du menu Fichier -&gt; Importer un appareil virtuel, s\u00e9lectionner le fichier <code>BDTools.ova</code> fourni et choisissez l'emplacement de destination.</li> <li>Une fois l'importation finie, v\u00e9rifier les param\u00e8tres de la machine virtuelle pour les ajuster \u00e0 la configuration de votre machine. Il est recommand\u00e9 d'utiliser 8G de RAM en gardant au moins 2G pour la machine h\u00f4te.</li> <li>D\u00e9marrer la machine virtuelle et se connecter avec l'utilisateur <code>uti</code>. Le mot de passe <code>=uti=</code>. Par d\u00e9faut, les services actifs sont HDFS, YARN et HBase. La commande suivante permet de s\u00e9lectionner les services n\u00e9cessaires \u00e0 Spark pour optimiser la gestion de la m\u00e9moire : <pre><code>sudo SelectService spark\n</code></pre></li> </ul> <p> Cette commande peut prendre quelques minutes selon la configuration de votre machine.</p> <p>Lancer le navigateur et aller sur la page http://localhost o\u00f9 l'\u00e9tat des services est affich\u00e9.</p>"},{"location":"index.html#autres-environnements","title":"Autres Environnements","text":"<p>Plusieurs autres alternatives sont disponibles afin de mettre en place un environnement pour reproduire les tests sur Spark. Deux environnements d'ex\u00e9cution sont pr\u00e9sent\u00e9s ci-apr\u00e8s :</p> <ul> <li> La machine virtuelle Cloudera QuickStart</li> <li> Un cluster Spark de 3 n\u0153uds avec Hadoop</li> <li> Databricks cloud (version d'essai)</li> </ul>"},{"location":"index.html#cloudera-quickstart-vm","title":"Cloudera QuickStart VM","text":"<p>Machine Virtuelle</p> <p>Les \u00e9tapes sont simples. Il suffit de :</p> <p> T\u00e9l\u00e9charger l'image de la machine qui convient \u00e0 votre logiciel de virtualisation (Oracle Virtual Box V6.1 ou Vmware) :</p> <ul> <li>Image Cloudera QuickStart 5.13 :</li> <li>pour VirtualBox</li> <li>pour vmware</li> </ul> <p> Ensuite, d\u00e9compresser puis importer l'image <code>.ovf</code> \u00e0 partir du logiciel de virtualisation.</p> <p> Enfin, d\u00e9marrer la machine qui est sous Centos 6.</p> <p>Docker</p> <p>Si vous pr\u00e9f\u00e9rez docker alors :</p> <p> Charger l'image docker :</p> <ul> <li>Option 1 :</li> </ul> <pre><code>docker pull cloudera/quickstart:latest\n</code></pre> <ul> <li>Option 2 : t\u00e9l\u00e9charger l'image docker et la charger avec :</li> </ul> <pre><code>docker load &lt; cloudera.tar.gz\n</code></pre> <p> D\u00e9marrer le container :</p> <pre><code>docker run -m 4G --memory-reservation 2G --memory-swap 8G --hostname=quickstart.cloudera --privileged=true -t -i --publish-all=true -p8888 -p8088 cloudera/quickstart /usr/bin/docker-quickstart\n</code></pre>"},{"location":"index.html#cluster-spark-avec-docker","title":"Cluster Spark avec Docker","text":"<p>Cr\u00e9er un cluster Spark compos\u00e9 de 3 n\u0153uds.</p> <p> T\u00e9l\u00e9charger et d\u00e9compresser l'archive spark-lab.zip</p> spark-lab.zip <p>Cette archieve contient :</p> <p><code>cluster.sh</code> : script pour la gestion du cluster sous Linux</p> <p><code>cluster.ps1</code> : script pour la gestion du cluster pour Windows Powershell</p> <p><code>spark-apps</code> : dossier contenant l'exemple wordcount. Il est sera mont\u00e9 comme un volume pour docker.</p> <p> Charger l'image docker</p> <pre><code>docker pull hhmida/sparkbase\n</code></pre> <p> D\u00e9ployer le cluster</p> Shell LinuxWindows Powershell <pre><code>./cluster.sh deploy\n</code></pre> <pre><code>cluster.ps1 deploy\n</code></pre> Options du script <pre><code>cluster.sh stop   # Arr\u00eater les containers \ncluster.sh start  # D\u00e9marrer les containers\ncluster.sh info   # Montre les URLs pour acc\u00e9der aux services\ncluster.sh deploy # Formater le cluster et red\u00e9ployer\n</code></pre> <p>Vu que les ports sont publi\u00e9s sur la machine h\u00f4te, <code>localhost</code> peut \u00eatre utilis\u00e9 avec les URLs des services.</p>"},{"location":"index.html#databricks","title":"Databricks","text":""},{"location":"rdd.html","title":"Manipuler les RDD","text":""},{"location":"rdd.html#manipuler-les-rdd","title":"Manipuler les RDD","text":"<p>Les RDD prennent en charge deux types d'op\u00e9rations: les transformations, qui cr\u00e9ent un nouvel RDD \u00e0 partir d'un RDD existant, et les actions, qui renvoient une valeur au programme Driver apr\u00e8s avoir ex\u00e9cut\u00e9 un calcul sur RDD. </p> <p>Toutes les transformations dans Spark sont paresseuses, en ce sens qu'elles ne calculent pas leurs r\u00e9sultats tout de suite. Au lieu de cela, ils se souviennent simplement des transformations appliqu\u00e9es \u00e0 un ensemble de donn\u00e9es de base (par exemple, un fichier). Les transformations ne sont calcul\u00e9es que lorsqu'une action n\u00e9cessite qu'un r\u00e9sultat soit renvoy\u00e9 au programme pilote.</p> <p>Par d\u00e9faut, chaque RDD transform\u00e9 peut \u00eatre recalcul\u00e9 chaque fois que vous ex\u00e9cutez une action dessus. Cependant, vous pouvez \u00e9galement conserver un RDD en m\u00e9moire \u00e0 l'aide de la m\u00e9thode persist (ou cache), auquel cas Spark conservera les \u00e9l\u00e9ments sur le cluster pour un acc\u00e8s beaucoup plus rapide la prochaine fois que vous l'interrogerez. Il existe \u00e9galement une prise en charge des RDD persistants sur disque ou r\u00e9pliqu\u00e9s sur plusieurs n\u0153uds.</p>"},{"location":"rdd.html#transformations","title":"Transformations","text":"TransformationDescription map(func)   Retourne un RDD avec les \u00e9l\u00e9ments obtenus de l'application de func sur les \u00e9l\u00e9ments deu RDD intial.  filter(func)   Construit un RDD avec les \u00e9l\u00e9ments sur lesquels func retourne True.  flatMap(func)   Comme map mais la fonction func retourne une liste d'\u00e9l\u00e9ments associ\u00e9e \u00e0 chaque \u00e9l\u00e9ment du RDD initial.  mapPartitions(func)  Similar to map, but runs separately on each partition (block) of the RDD, so func must be of type     Iterator&lt;T&gt; =&gt; Iterator&lt;U&gt; when running on an RDD of type T.  mapPartitionsWithIndex(func)   Comme mapPartition et fournit le num\u00e9ro de la partition \u00e0 utiliser.    sample(withReplacement, fraction, seed)   S\u00e9lectionner une fraction des donn\u00e9es, avec ou sans remise.  union(otherDataset), intersection  Op\u00e9rations ensemblistes.  distinct([numPartitions]))   Retourne un RDD contenant les valeurs uniques. reduceByKey(func, [numPartitions])   Appel\u00e9e sur un RDD de paires (K, V) et retourne aussi un RDD de apires (K, V) o\u00f9 les valeurs sont agr\u00e9g\u00e9es selon la fonction func. Le nombre de t\u00e2ches reduce est d\u00e9termin\u00e9e par le second param\u00e8tre.  aggregateByKey(zeroValue)(seqOp, combOp, [numPartitions])   Permet d'obtenir un r\u00e9sultat d'agr\u00e9gation de type diff\u00e9rent de celui des valeurs agr\u00e9g\u00e9es.  sortByKey([ascending], [numPartitions])  Tri selon la cl\u00e9. join(otherDataset, [numPartitions])   Jointure de deux RDDs de paires (K, V) and (K, W) qui retourne une RDD de paires (K, (V, W)). La jointure externe est r\u00e9alis\u00e9e avec <code>leftOuterJoin</code>, <code>rightOuterJoin</code>, et <code>fullOuterJoin</code>.    cartesian(otherDataset)   Produit cart\u00e9sien de deux RDDs.  coalesce(numPartitions)  Diminuer le nombre de partitions. Utile suite \u00e0 un filtrage du RDD."},{"location":"rdd.html#actions","title":"Actions","text":"ActionDescription reduce(func)   Applique la fonction d'agr\u00e9gation func.  collect()   Retourne les \u00e9l\u00e9ments du RDD sous la frorme de liste au Driver.  count()   Retourne le nombre d'\u00e9l\u00e9ments du RDD.  first()   Retourne le premier \u00e9l\u00e9ment (\u00e9quivalent \u00e0 take(1)).  take(n)   Retourne un tableau des n premiers \u00e9l\u00e9ments.  takeSample(withReplacement, num, [seed])   Combinaison de <code>sample</code> et <code>take</code>. takeOrdered(n, [ordering])   Return the first n elements of the RDD using either their natural order or a custom comparator.  saveAsTextFile(path)   Enregistre les \u00e9l\u00e9ments dans un fichier texte (chaque \u00e9l\u00e9ment est converti en cha\u00eene de caract\u00e8res).  countByKey()   Retourne le nombre d'\u00e9l\u00e9ments pour chaque cl\u00e9.  foreach(func)   Applique une fonction sur chaque \u00e9l\u00e9ment sans retour. Par exemple la sauvegarde dans un fichier."},{"location":"rdd.html#persistance-des-rdd","title":"Persistance des RDD","text":"<p>C'est la cl\u00e9 pour le gain de performance lors des traitements it\u00e9ratifs sur les RDDs. Il s'agit de garder en m\u00e9moire les donn\u00e9es pour de futures manipulations (voir l'exemple dans la section pr\u00e9c\u00e9dente).</p> <p>Pour activer la persistance sur un RDD, il suffit d'appleer <code>persist()</code> ou <code>cache()</code>. Il est possible de d\u00e9finir des niveaux de persistance avec <code>persist(StorageLevel.&lt;niveau&gt;)</code> ou niveau peut \u00eatre :</p> <ul> <li><code>MEMORY_ONLY</code> : En m\u00e9moire. Si la m\u00e9moire est insuffisante les partitions non charg\u00e9es en m\u00e9moire seront calcul\u00e9es \u00e0 la vol\u00e9e au besoin. C'est le niveau par d\u00e9faut (utilis\u00e9 avec cache()).</li> <li><code>MEMORY_AND_DISK</code> Les partitions qui d\u00e9apssent la capacit\u00e9 de la m\u00e9moire sont stock\u00e9es sur disque.</li> <li><code>MEMORY_ONLY_SER</code> et <code>MEMORY_AND_DISK_SER</code> (Java and Scala) : idem. mais les donn\u00e9es sont s\u00e9rialis\u00e9s. Avec Python, les RDD sont toujours s\u00e9rialis\u00e9es au format pickle.</li> <li><code>DISK_ONLY</code> : stockage sur disque seulement.</li> <li><code>MEMORY_ONLY_2</code>, <code>MEMORY_AND_DISK_2</code>, etc. : avec r\u00e9plication sur 2 noeuds.</li> </ul>"},{"location":"rdd.html#exemples","title":"Exemples","text":"<p>Ouvrir dans un nouvel onglet</p> <p>T\u00e9l\u00e9charger le notebook DemoRDD.ipynb </p> <p>T\u00e9l\u00e9charger le fichier zipcodes.csv </p>"},{"location":"rdd.html#exercice","title":"Exercice","text":"<ol> <li>T\u00e9l\u00e9charger les donn\u00e9es sur :<ol> <li>Les ventes mensuelles \u00e0 l'export de d\u00e9riv\u00e9s de Phosphate : vente_export_der_ph_mois.csv</li> <li>La production mensuelle de d\u00e9riv\u00e9s de Phosphate : prod_der_phosphate_mois.csv</li> </ol> </li> <li>En utilisant les RDD de Spark, \u00e9crire un programme Python permettant de r\u00e9pondre aux deux questions suivantes :<ol> <li>Quelle est la plus grande production mensuelle de NPK (quantit\u00e9 et date).</li> <li>Les productions annuelles de chaque produit d\u00e9riv\u00e9.</li> <li>Le volume des productions annuelles tous produits confondus.</li> <li>Les ventes annuelles de chaque produit d\u00e9riv\u00e9.</li> <li>La diff\u00e9rence entre (productions annuelles - ventes \u00e0 l'export annuelles) de chaque produit.</li> </ol> </li> </ol> Directives \u00e0 suivre <ul> <li>Supprimer l'ent\u00eate du fichier csv avec les RDD (Question 2) :</li> </ul> <pre><code>from itertools import islice\nrddP = sc.textFile('prod_der_phosphate_mois.csv.csv').cache()\nrddP = rddP.mapPartitionsWithIndex(lambda idx, iter: islice(iter,1,None) if (idx == 0) else iter)\n</code></pre> <ul> <li>Pr\u00e9voir un traitement pour les valeurs manquantes.</li> <li>Enregistrer chaque r\u00e9ponse individuellement. (<code>rdd.saveAsTextFile(...)</code>)</li> </ul>"},{"location":"sparksql.html","title":"SparkSQL","text":""},{"location":"sparksql.html#sparksql","title":"SparkSQL","text":"<p>SparkSQL est un module Spark qui se base sur les RDD permettant de les d\u00f4ter d'une structure ou sch\u00e9ma et les interroger \u00e0 travers le SQL en utilisant les DataFrames.</p>"},{"location":"sparksql.html#contexte-sql","title":"Contexte SQL","text":"<p>L'acc\u00e8s aux objets et m\u00e9thodes de l'API SparkSQL est possible \u00e0 partir d'un objet SparkSession ou SQLContext.</p> Contexte SQL SparkSessionSQLContext <pre><code>from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('Exemple SparkSQL').getOrCreate()\n</code></pre> <pre><code>from pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsc = SparkContext(conf = SparkConf().setAppName('Exemple SparkSQL'))\nsqlc = SQLContext(sc)\n</code></pre>"},{"location":"sparksql.html#creation-de-dataframe","title":"Cr\u00e9ation de DataFrame","text":"<p>Elle peut \u00eatre r\u00e9alis\u00e9e de 2 fa\u00e7ons :</p> <ul> <li>\u00e0 partir de RDDs existantes ou DataFrame pandas : <code>spark.createDataFrame(data, schema=None, samplingRatio=None, verifySchema=True)</code> ou <code>rdd.toDF(schema=None, samplingRatio=None)</code></li> <li>\u00e0 partir de sources externes (json, csv, ...) : <code>spark.read.&lt;format&gt;</code> (semblables aux m\u00e9thodes de la biblioth\u00e8que pandas)</li> </ul> <pre><code>rdd = spark.sparkContext.parallelize([('Stylo',0.625, 10),('Rame papier',9.200, 5)])\ndf = spark.createDataFrame(rdd, schema=['Produit', 'Prix', 'Qte'])\n</code></pre>"},{"location":"sparksql.html#commandes-sql","title":"Commandes SQL","text":"<p>Afin de pouvoir utiliser le DataFrame dans les requ\u00eates SQL, il faut la d\u00e9clarer comme une vue temporaire.</p> Cr\u00e9ation de vue temporaire <pre><code>df.createOrReplaceTempView(\"table1\")\n</code></pre> <p>Par la suite, lancer une requ\u00eate SQL est fait avec la m\u00e9thode <code>spark.sql</code> :</p> Cr\u00e9ation de vue temporaire <pre><code>spark.sql(\"SELECT * FROM table1\")\n</code></pre>"},{"location":"sparksql.html#api-dataframe","title":"API DataFrame","text":"<p>Elle comporte une liste assez riche d'op\u00e9rations. Parmi lesquelles, nous pouvons citer :</p> <ol> <li> <p>Op\u00e9rations de type SQL :</p> <ul> <li><code>df.select(col1, col2, ...)</code></li> <li><code>df.where(condition)</code></li> <li><code>df.groupBy(col1, col2, ...)</code></li> <li><code>df.orderBy(cols, ascending, ...)</code></li> <li><code>df.join(other, on=None, how=None)</code></li> <li><code>df.limit(num)</code></li> <li><code>df.agg(...)</code></li> </ul> </li> <li> <p>Autres op\u00e9rations :</p> <ul> <li><code>df.show(n)</code></li> <li><code>df.printSchema()</code></li> <li><code>df.describe(col1, col2, ...)</code></li> <li><code>df.drop(col1, col2, ...)</code></li> <li><code>df.dropna(how='any', thresh=None, subset=None)</code></li> <li><code>df.withColumn(colName, col)</code></li> </ul> </li> </ol>"},{"location":"sparksql.html#exemples","title":"Exemples","text":"<p>Ouvrir dans un nouvel onglet</p> <p>T\u00e9l\u00e9charger le notebook DemoSparkSQL.ipynb </p> <p>T\u00e9l\u00e9charger le fichier zipcodes.csv </p>"},{"location":"sparksql.html#exercices","title":"Exercices","text":""},{"location":"sparksql.html#exercice-1","title":"Exercice 1","text":"<p>Reprendre l'exercice de la section RDD avec SparkSQL.</p>"},{"location":"sparksql.html#exercice-2","title":"Exercice 2","text":"<p>T\u00e9l\u00e9charger le Notebook et suivre les instructions pour compl\u00e9ter cette \u00e9tude de cas financi\u00e8re.</p>"},{"location":"streaming.html","title":"Spark Streaming","text":""},{"location":"streaming.html#spark-streaming","title":"Spark Streaming","text":"<p>Spark offre la possibilit\u00e9 de traiter des donn\u00e9es en flux \u00e0 partir de plusieurs sources (kafka, twitter, TCP, ...). Spark transforme le flux continue de donn\u00e9es en batchs \u00e0 des intervalles de temps r\u00e9guliers. Il cr\u00e9e ainsi une s\u00e9quence continue de RDDs de m\u00eame type accessibles \u00e0 travers la classe <code>DStream</code>. Cette classe cr\u00e9e un RDD \u00e0 des intervalles de temps r\u00e9guliers. </p> <p>Le point d'entr\u00e9e aux fonctionnalit\u00e9s de Streaming de Spark est le <code>StreamingContext</code>. Il permet de cr\u00e9er des instances de <code>DStream</code>.</p>"},{"location":"streaming.html#streamingcontext","title":"StreamingContext","text":""},{"location":"streaming.html#creation","title":"Cr\u00e9ation","text":"<pre><code>StreamingContext(sparkContext, batchDuration=None)\n</code></pre> Exemple <pre><code>from pyspark.sql import SparkSession\nfrom pyspark.streaming import StreamingContext\nspark = SparkSession.builder.appName('Exemple Streaming').getOrCreate()\nssc = StreamingContext(spark.sparkContext, 10)\n</code></pre>"},{"location":"streaming.html#principales-methodes","title":"Principales M\u00e9thodes","text":"<ul> <li><code>socketTextStream(hostname, port)</code> : flux de donn\u00e9es \u00e0 partir de socket TCP (codage UTF8 et \\n pour fin de ligne).</li> <li><code>textFileStream(directory)</code> : surveille un dossier et lit les nouveaux fichiers dans le format texte.</li> <li><code>binaryRecordsStream(directory, recordLength)</code> : comme textFileStream mais les fichiers sont lus dans le format binaire avec des enregistrements de taille fixe = <code>recordLength</code>.</li> <li><code>queueStream(rdds, oneAtATime=True, default=None)</code> : une file de RDDs comme source de donn\u00e9es.</li> <li><code>start()</code> : d\u00e9marre le traitement des flux.</li> <li><code>stop(stopSparkContext=True, stopGraceFully=False)</code> : arr\u00eat du traitement des flux.</li> <li><code>awaitTermination(timeout=None)</code> : attente de la fin d'ex\u00e9cution.</li> </ul>"},{"location":"streaming.html#dstream-discretized-stream","title":"DStream (Discretized Stream)","text":"<p>Une instance DStream est caract\u00e9ris\u00e9e par :</p> <ul> <li>La liste d'autres DStreams sur lesquelles elle d\u00e9pend.</li> <li>Un intervalle au bout duquel elle g\u00e9n\u00e8re un RDD.</li> <li>Une fonction utilis\u00e9e pour la g\u00e9n\u00e9ration du RDD.</li> </ul> <p>DStream poss\u00e8de plusieurs m\u00e9thodes communes avec RDD comme : `map(), flatMap(), reduce(), reduceByKey(), filter(), count(), join(), ... Elles g\u00e9n\u00e9rent \u00e9galement des DStreams.</p> <p>Autres m\u00e9thodes :</p> <ul> <li><code>pprint([num])</code> : affiche les n premiers \u00e9l\u00e9ments.</li> <li><code>window(windowDuration[, slideDuration])</code> : cr\u00e9e un nouveau DStream o\u00f9 chaque RDD contient les \u00e9l\u00e9ments re\u00e7us dans une fen\u00eatre de temps sur le DStream.</li> </ul>"},{"location":"streaming.html#exemples","title":"Exemples","text":""},{"location":"streaming.html#spark-streaming-avec-sockettextstream","title":"Spark Streaming avec socketTextStream","text":"<p> D\u00e9marrer netcat dans un premier terminal</p> <pre><code>nc -l -p 9999\n</code></pre> <p> Cr\u00e9er le fichier source <code>network_wordcount.py</code></p> network_wordcount.py <pre><code>import sys\n\nfrom pyspark import SparkContext\nfrom pyspark.streaming import StreamingContext\n\nif __name__ == \"__main__\":\n    if len(sys.argv) != 3:\n        print(\"Usage: network_wordcount.py &lt;hostname&gt; &lt;port&gt;\", file=sys.stderr)\n        sys.exit(-1)\n    sc = SparkContext(appName=\"PythonStreamingNetworkWordCount\")\n    ssc = StreamingContext(sc, 5)\n\n    lines = ssc.socketTextStream(sys.argv[1], int(sys.argv[2]))\n    counts = lines.flatMap(lambda line: line.split(\" \"))\\\n                .map(lambda word: (word, 1))\\\n                .reduceByKey(lambda a, b: a+b)\n    counts.pprint()\n\n    ssc.start()\n    ssc.awaitTermination()\n</code></pre> <p> D\u00e9marrer l'application Spark dans un autre ternminal</p> <pre><code>spark-submit network_wordcount.py localhost 9999\n</code></pre> <p> Commencer \u00e0 taper du texte dans le premier terminal et v\u00e9rifier le second terninal</p>"},{"location":"streaming.html#spark-streaming-avec-textfilestream","title":"Spark Streaming avec textFileStream","text":"<p> Cr\u00e9er un dossier <code>text</code></p> <p> Cr\u00e9er le fichier source <code>folder_wordcount.py</code></p> folder_wordcount.py <pre><code>from pyspark import SparkContext\nfrom pyspark.streaming import StreamingContext\n\nif __name__ == \"__main__\":\n    sc = SparkContext(appName=\"PythonStreamingNetworkWordCount\")\n    ssc = StreamingContext(sc, 5)\n\n    lines = ssc.textFileStream(\"text/\")\n    counts = lines.flatMap(lambda line: line.split(\" \"))\\\n                .map(lambda word: (word, 1))\\\n                .reduceByKey(lambda a, b: a+b)\n    counts.pprint()\n\n    ssc.start()\n    ssc.awaitTermination()\n</code></pre> <p> D\u00e9marrer l'application Spark</p> <pre><code>spark-submit folder_wordcount.py\n</code></pre> <p> Ajouter des fichiers dans le dossier <code>text</code> et v\u00e9rifier l'affichage sur le terminal</p>"},{"location":"streaming.html#spark-streaming-avec-kafka","title":"Spark Streaming avec Kafka","text":""},{"location":"streaming.html#installation-de-kafka","title":"Installation de Kafka","text":"<p>Si kafka n'est pas install\u00e9 alors suivre les \u00e9tapes suivantes :</p> <p> T\u00e9l\u00e9charger les binaires de Kafka</p> <p><code>wget https://archive.apache.org/dist/kafka/0.10.2.2/kafka_2.12-0.10.2.2.tgz</code></p> <p> D\u00e9compresser et renommer le dossier obtenu</p> <pre><code>tar -xzf kafka_2.12-0.10.2.2.tgz\nmv kafka_2.12-0.10.2.2.tgz kafka\n</code></pre> <p> Ajouter l'emplacement de Kafka \u00e0 la variable PATH dans .bashrc ou .profile</p> <pre><code>echo \"PATH=$PATH:~/kafka/bin\" &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre>"},{"location":"streaming.html#demarrer-lexemple","title":"D\u00e9marrer l'exemple","text":"<p>Les commandes sont ex\u00e9cut\u00e9es \u00e0 partir du dossier kafka</p> <p> D\u00e9marrer Zookeeper</p> <pre><code>bin/zookeeper-server-start.sh config/zookeeper.properties\n</code></pre> Zookeeper <p>ZooKeeper est un service de coordination pour les applications distribu\u00e9es.ZooKeeper enregistrer les m\u00e9tadonn\u00e9es concernant le cluster (h\u00f4tes, ports, ...). Il est fourni avec Kafka.</p> <p> D\u00e9marrer le broker Kafka dans une nouvelle session</p> <pre><code>bin/kafka-server-start.sh config/server.properties\n</code></pre> <p> Cr\u00e9er un topic sur Kafka Dans une nouvelle session :</p> <pre><code>bin/kafka-topics.sh --create --zookeeper localhost:2181 \\\n--replication-factor 1 \\\n--partitions 1 \\\n--topic wordcounttopic\n</code></pre> Topic Kafka <p>Les messages Kafka sont organis\u00e9s en topics. Ces derniers sont partitionn\u00e9s et r\u00e9pliqu\u00e9es sur de multiples brokers \u00e0 travers le cluster. Un processus Producer envoie les messages au topic et un processus consumer les lit. Dans cet exemple, le producer est le programme fourni par Kafka, le consumer est Spark. Pour voir les topics disponibles :</p> <pre><code>bin/kafka-topics.sh --zookeeper localhost:2181 --list\n</code></pre> <p> D\u00e9marrer le producer dans une nouvelle session</p> <pre><code>bin/kafka-console-producer.sh --broker-list localhost:9092 --topic wordcounttopic\n</code></pre> <p> D\u00e9marrer l'application Spark</p> <pre><code>spark-submit --master yarn --deploy-mode client --conf \"spark.dynamicAllocation.enabled=false\" \\\n--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.0 kafka_wordcount.py \\\nlocalhost:2181 wordcounttopic\n</code></pre> kafka_wordcount.py <pre><code>from __future__ import print_function\nimport sys\nfrom pyspark import SparkContext\nfrom pyspark.streaming import StreamingContext\nfrom pyspark.streaming.kafka import KafkaUtils\nif __name__ == \"__main__\":\n    if len(sys.argv) != 3:\n        print(\"Usage: kafka_wordcount.py &lt;zk&gt; &lt;topic&gt;\", file=sys.stderr)\n        sys.exit(-1)\n    sc = SparkContext(appName=\"PythonStreamingKafkaWordCount\")\n    ssc = StreamingContext(sc, 10)\n    zkQuorum, topic = sys.argv[1:]\n    kvs = KafkaUtils.createStream(ssc, zkQuorum, \"spark-streamingconsumer\", {topic: 1})\n    lines = kvs.map(lambda x: x[1])\n    counts = lines.flatMap(lambda line: line.split(\" \"))\\\n        .map(lambda word:(word, 1)).reduceByKey(lambda a, b: a+b)\n    counts.pprint()\n    ssc.start()\n    ssc.awaitTermination()\n</code></pre> <p> Taper du texte dans la session du producer et inspecter l'affichage obtenu par</p> Messages de log <p>Si Spark affiche trop de messages d'information, vous pouvez r\u00e9duire le niveau de journalisation en modifiant la ligne du fichier <code>spark/conf/log4j.properties</code> :</p> <pre><code>log4j.rootCategory = ERROR, console\n</code></pre>"},{"location":"streaming.html#structured-streaming","title":"Structured Streaming","text":"<p>Spark Structured Streaming offre un moteur de traitement de flux au-dessus de Spark SQL. Il permet d'exprimer les traitements sur les flux de la m\u00eame fa\u00e7on que sur des donn\u00e9es statiques. Spark SQL se charge de l'ex\u00e9cution continue et incr\u00e9mentale et la mise \u00e0 jour du r\u00e9sultat final. Spark r\u00e9alise ce traitement avec une latence de l'ordre de 1 milliseconde. Les flux de donn\u00e9es sont repr\u00e9sent\u00e9es sous forme de <code>DataFrame</code> qui peut \u00eatre assimil\u00e9e \u00e0 une table de taille illimit\u00e9e dont le contenu augmente au cours du temps avec la r\u00e9ception du flux.</p> <p>L'API Spark, repose sur la classe <code>DataStreamReder</code> pour charger un <code>DataFrame</code> de flux \u00e0 partir d'une source  cpmme HDFS,  Kafka, Kinesis, ...).  Le traitement sur ce flux est exprim\u00e9 via une requ\u00eate <code>StreamingQuery</code>. L'\u00e9criture d'un flux est prise en charge par la classe <code>DataStreamWriter</code> obtenu \u00e0 partir de <code>DataFrame.writeStream</code>.</p> <p>Le mod\u00e8le de programmation est repr\u00e9sent\u00e9 dans la figure suivante : </p> <p>L'instance <code>DataStremReader</code> est obtenu \u00e0 partir de <code>SparkSession</code> par : <code>readStream</code>. Elle poss\u00e8de les m\u00e9thodes :</p> <ul> <li><code>format(source)</code> : sp\u00e9cifie la source (socket, kafka, ...)</li> <li><code>option(key, value)</code> : ajoute une configuration pour la source</li> <li><code>csv(path[, schema, sep, encoding, quote, \u2026])</code> : charge un fichier csv</li> <li><code>text(path[, wholetext, lineSep, \u2026])</code> : charge un fichier texte</li> <li><code>load([path, format, schema])</code> : charge le flux de la source et le retourne sous la forme de DataFrame</li> </ul> <p>Et voici quelques m\u00e9thode la classe <code>DataStreamWriter</code> :</p> <ul> <li><code>format(source)</code> : sp\u00e9cifie la source (socket, kafka, ...)</li> <li><code>option(key, value)</code> : ajoute une configuration pour la source</li> <li><code>start([path, format, outputMode, \u2026])</code> : charge le flux de la source et le retourne sous la forme de DataFrame</li> <li> <p><code>outputMode(outputMode)</code> : append, complete, ...</p> </li> <li> <p><code>awaitTermination([timeout])</code></p> </li> <li><code>stop()</code></li> <li><code>processAllAvailable()</code></li> </ul>"},{"location":"streaming.html#exemples_1","title":"Exemples","text":""},{"location":"streaming.html#source-socket","title":"Source Socket","text":"<p>Suivre les m\u00eames \u00e9tapes que dans l'exemple <code>socketTextStream</code>. </p> structured_network_wordcount.py <pre><code>from pyspark.sql import SparkSession\nfrom pyspark.streaming import StreamingContext\nfrom pyspark.sql.functions import explode\nfrom pyspark.sql.functions import split\n\nspark = SparkSession.builder.appName('Spark Structured Streaming').getOrCreate()\n# Create SPARK Context\nsc = spark.sparkContext\nsc.setLogLevel(\"WARN\")\n## Create Streaming context , with 10 second interval \nssc = StreamingContext(sc,  10) \n# Create DataFrame representing the stream of input lines from connection to localhost:9999\nlines = spark \\\n    .readStream \\\n    .format(\"socket\") \\\n    .option(\"host\", \"localhost\") \\\n    .option(\"port\", 9999) \\\n    .load()\n\n# Split the lines into words\nwords = lines.select(\nexplode(\n    split(lines.value, \" \")\n).alias(\"word\")\n)\n\n# Generate running word count\nwordCounts = words.groupBy(\"word\").count()\n# Start running the query that prints the running counts to the console\nquery = wordCounts \\\n    .writeStream \\\n    .outputMode(\"complete\") \\\n    .format(\"console\") \\\n    .start()\nquery.awaitTermination()\n</code></pre> <pre><code>spark-submit structured_network_wordcount.py\n</code></pre>"},{"location":"streaming.html#source-fichiers-csv-dans-un-dossier","title":"Source fichiers csv dans un dossier","text":"<p>Dans cet exemple, nous allons simuler des transactions sur un site de vente en ligne qui sont export\u00e9s vers un dossier <code>orders</code> en fin de journ\u00e9e.</p> <p> Cr\u00e9er le dossier <code>orders</code></p> <p> Cr\u00e9er deux fichiers CSV</p> 1.csv <pre><code>num,client,montant\n1,1,145\n2,2,414\n3,1,150\n4,1,10\n</code></pre> 2.csv <pre><code>num,client,montant\n5,4,145\n6,3,414\n7,2,150\n8,2,10\n</code></pre> <p> Cr\u00e9er le fichier source <code>csvstreaming.py</code></p> csvstreaming.py <pre><code>from pyspark.sql import SparkSession\nfrom pyspark.streaming import StreamingContext\n\nspark = SparkSession.builder.appName('Spark Structured Streaming').getOrCreate()\nspark.sparkContext.setLogLevel(\"WARN\")\ndf = spark \\\n    .readStream \\\n    .schema(\"num int,client int,montant int\")\\\n    .csv(\"orders/\", header=True)\n\ncount_orders = df.groupby(\"client\").count()\n# Start running the query that prints the running counts to the console\nquery =count_orders \\\n    .writeStream \\\n    .outputMode(\"complete\") \\\n    .format(\"console\") \\\n    .start()\nquery.awaitTermination()\n</code></pre> <p> Soumettre l'application Spark</p> <p><code>spark-submit csvstreaming.py</code></p> <p> Ajouter les deux fichier un \u00e0 un et surveiller l'affichage</p> Atelier sur Gitpod <pre><code>Vous pouvez d\u00e9marrer cette activit\u00e9 sur Gitpod en cliquant ici : [![Open in Gitpod](https://gitpod.io/button/open-in-gitpod.svg)](https://gitpod.io/#https://github.com/hhmida/SparkStreaming)\n</code></pre>"},{"location":"streaming.html#exercice","title":"Exercice","text":"<p>Cr\u00e9er une application Spark Streaming permettant de surveiller les logs d'un serveur web et calculer le nombre d'erreurs HTTP (comme 404, 501, ...). Le r\u00e9sultat sera enregistr\u00e9 dans le dossier <code>output</code>.</p>"},{"location":"submitting.html","title":"Cr\u00e9er des applications Spark","text":""},{"location":"submitting.html#creer-des-applications-spark","title":"Cr\u00e9er des applications Spark","text":"<p>Une application Spark est coordonn\u00e9e par l'objet SparkContext cr\u00e9\u00e9 dans le programme principal appel\u00e9 <code>Driver</code>.  Ce dernier se connecte \u00e0 un gestionnaire de cluster (Spark, Yarn, Mesos, Kubernetes) qui lui allouera les ressources n\u00e9cessaires. Le Driver obtient alors les Executors (processus sur les n\u0153uds du cluster) et par la suite leur envoie le code puis les t\u00e2ches \u00e0 r\u00e9aliser. (voir figure ci-apr\u00e8s)</p> <p></p> <p>Le lancement d'une application Spark peut \u00eatre fait selon 2 m\u00e9thodes :</p> <ul> <li>Une ex\u00e9cution interactive</li> <li>Soumission de jobs \u00e0 un gestionnaire : Spark Standalone, YARN, Meos ou Kubernetes</li> </ul>"},{"location":"submitting.html#execution-interactive","title":"Ex\u00e9cution interactive","text":"<p>C'est une ex\u00e9cution sur le n\u0153ud local dont l'utilit\u00e9 est de tester l'application Spark.</p>"},{"location":"submitting.html#avec-le-shell-pyspark","title":"Avec le shell pyspark","text":"<p>Spark dispose d'un shell pour interpr\u00e9ter les instruction Python qui peut \u00eatre invoqu\u00e9 avec la commande <code>pyspark</code>. Un objet SparContext est alors cr\u00e9\u00e9 et plac\u00e9 dans la variable <code>sc</code>. Le programme est ex\u00e9cut\u00e9 en simulant un nombre de n\u0153uds \u00e9gal au nombre de cores logiques du processeur (voir <code>Local[*]</code> plus bas).</p> <p>Pour voir l'\u00e9tat du job, aller sur : http://localhost:4040. Cette interface n'est disponible que pendant <code>pyspark</code> est en cours d'ex\u00e9cution.</p>"},{"location":"submitting.html#exemple-wordcount","title":"Exemple Wordcount","text":"<p> Mettre le fichier <code>shakespeare.txt</code> sous HDFS.</p> <pre><code>hadoop fs -put shakespeare.txt\n</code></pre> <p> Lancer <code>pyspark</code></p> <pre><code>pyspark\n</code></pre> <p> Ex\u00e9cuter les instructions suivantes une \u00e0 une :</p> <pre><code>texte = sc.textFile('shakespeare.txt')\ncount = texte.flatMap(lambda l:l.split()).map(lambda w:(w,1)).reduceByKey(lambda a,b:a+b)\ncount.collect()\n</code></pre> <p></p>"},{"location":"submitting.html#notebook-jupyter","title":"Notebook jupyter","text":"<p> T\u00e9l\u00e9charger le notebook wordcount.ipynb </p> <p> Lancer Jupyter depuis le dossier contenant <code>wordcount.ipynb</code></p> <pre><code>jupyter notebook\n</code></pre> <p> Acc\u00e9der au notebook jupyter <code>wordcount.ipynb</code></p> <p> Spark UI</p> <p>Surveiller le job sur http://localhost:4040 remarquer que l'ex\u00e9cution ne se d\u00e9clenche r\u00e9ellement qu'apr\u00e8s l'appel de la m\u00e9thode <code>collect()</code>.</p> <p></p> <p>Parmi les onglets de Spark UI :</p> <ul> <li>Jobs : Un job est l\u2019ex\u00e9cution d\u2019une cha\u00eene de traitements (workflow) dans un environnement distribu\u00e9. Le workflow peut \u00eatre visualis\u00e9 \u00e0 partir de DAG Visualization. Le nombre de jobs est \u00e9agal au nombre d'actions sur les RDDs. Dans cet exemple, l\u2019ex\u00e9cution s\u2019est faite en deux \u00e9tapes (Stage). La premi\u00e8re comprend les transformations textFile, flatMap et map, la seconde la transformation reduceByKey. les deux \u00e9tapes sont s\u00e9par\u00e9es par une phase de shuffle. L\u2019ex\u00e9cution d\u2019une \u00e9tape se fait par un ensemble de t\u00e2ches, une par machine h\u00e9bergeant un fragment du RDD. </li> </ul> <p> </p> <ul> <li>Stages : Il montre de nombreuses statistiques sur le temps d\u2019ex\u00e9cution, le volume des donn\u00e9es \u00e9chang\u00e9es, ... de chaque \u00e9tape. </li> </ul> <p></p> <ul> <li>Storage : Contient les donn\u00e9es en m\u00e9moire pendant l'ex\u00e9cution. On peut demander explicitement de garder en m\u00e9moire un RDD avec <code>cache()</code> ou <code>persist()</code>.</li> </ul> persist() <p>Inspecter l'onglet Storage apr\u00e8s l'ex\u00e9cution de la cellule avec le code <code>count.persist()</code> puis comparer les temps d'ex\u00e9cution des deux instructions <code>collect()</code>.</p> <p> </p> Google Colab <p>Pour ex\u00e9cuter le Notebook pr\u00e9c\u00e9dent en utilisant Google Colab, suivre les \u00e9tapes suivantes :</p> <ol> <li>Acc\u00e9der \u00e0 Google Colab https://colab.research.google.com/</li> <li>Importer le Notebook <code>wordcount.ipynb</code> \u00e0 partir du menu <code>Fichier</code> puis <code>Importer le notebook</code></li> <li>Ex\u00e9cuter en premier lieu la cellule contenant <code>!pip install  pyspark</code> (\u00e0 ins\u00e9rer si elle n'est pas existante)</li> <li>Importer le fichier <code>shakespeare.txt</code> en ex\u00e9cutant <code>!wget https://raw.githubusercontent.com/hhmida/tp-spark/main/assets/shakespeare.txt</code></li> <li>Ex\u00e9cuter les diff\u00e9rentes cellules</li> </ol>"},{"location":"submitting.html#soumission-avec-spark-submit","title":"Soumission avec spark-submit","text":"<p>C'est un script Spark pour soumettre des jobs dans les langages accept\u00e9s sans interaction. Avec les param\u00e8tres il est possible de choisir le mode d'ex\u00e9cution, la r\u00e9servation des ressources, la configuration Spark...</p> <p>La diff\u00e9rence, par rapport \u00e0 pyspark, est qu'il faut cr\u00e9er une instance <code>SparkContext</code> dans le programme Python soumis :</p> <pre><code>sc = SparkContext(SparkConf(...))\n</code></pre> spark-submit <pre><code>spark-submit \\\n--master &lt;master-url&gt; \\\n--deploy-mode &lt;client|cluster&gt; \\\n--conf &lt;key&lt;=&lt;value&gt; \\\n--driver-memory &lt;value&gt;g \\\n--executor-memory &lt;value&gt;g \\\n--executor-cores &lt;number of cores&gt;  \\\n--jars  &lt;comma separated dependencies&gt; \\\n--py-files &lt;comma separated dependencies&gt; \\\n--class &lt;main-class&gt; \\\n&lt;application-jar-py&gt; \\\n[application-arguments]\n</code></pre>"},{"location":"submitting.html#en-local","title":"En local","text":"<p>Mode similaire \u00e0 <code>pyspark</code> sans l'interactivit\u00e9.</p> Mode local Commande <pre><code>spark-submit --master local[*] wordcount.py\n</code></pre> UI <p>http://localhost:4040</p>"},{"location":"submitting.html#spark-standalone-cluster","title":"Spark standalone cluster","text":"Mode Standalone Cluster Commande <pre><code>spark-submit --master spark://localhost:7077\n</code></pre> UI <p>http://localhost:8888</p> <p></p>"},{"location":"submitting.html#yarn","title":"yarn","text":"Mode Yarn Commande <pre><code>spark-submit --master yarn --deploy-mode cluster wordcount.py\n</code></pre> UI <p>La surveillance est prise en charge avec Hadoop. http://localhost:8080</p> <p></p>"}]}