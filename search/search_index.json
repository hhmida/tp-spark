{"config":{"indexing":"full","lang":["en","fr"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-\\.]+"},"docs":[{"location":"index.html","text":"Calcul distribu\u00e9 avec Spark \u00b6 Objectifs \u00b6 Utiliser Spark sous Hadoop Cr\u00e9er des programmes avec l'API Spark Manipuler les diff\u00e9rentes structures de donn\u00e9es Spark : RDD, DataFrame Interroger les donn\u00e9es avec SparKSQL Traiter des flux de donn\u00e9es avec Spark Streaming Cr\u00e9er des pipeline ML Outils Image de machine virtuelle avec les outils pr\u00e9-install\u00e9s. C'est une image cr\u00e9\u00e9e par Pierre Nezric \u00e0 laquelle sont ajout\u00e9s jupyter et mrjob . Elle contient les outils suivants : Hadoop 2.7.3 Spark 2.1.1 Pig 0.15.0 Hive 1.2.1 HBase 1.1.9 Cassandra Elasticsearch et Kibana Zookeeper 3.4.6 Sources et r\u00e9f\u00e9rences Documentation Spark Pr\u00e9sentation de Spark \u00b6 Avec MapReduce, la sp\u00e9cification de l\u2019it\u00e9ration reste \u00e0 la charge du programmeur; il faut stocker le r\u00e9sultat d\u2019un premier job dans une collection interm\u00e9diaire et r\u00e9iterer le job en prenant la collection interm\u00e9diaire comme source. C\u2019est laborieux pour l\u2019implantation, et surtout tr\u00e8s peu efficace quand la collection interm\u00e9diaire est grande. Le processus de s\u00e9rialisation/d\u00e9s\u00e9rialisation sur disque propre \u00e0 la gestion de la reprise sur panne en MapReduce entra\u00eene des performances m\u00e9diocres. Dans Spark, la m\u00e9thode est tr\u00e8s diff\u00e9rente. Elle consiste \u00e0 placer ces jeux de donn\u00e9es en m\u00e9moire RAM et \u00e0 \u00e9viter la p\u00e9nalit\u00e9 des \u00e9critures sur le disque. Spark offre un \u00e9cosyst\u00e8me de librairies couvrant : L'apprentissage automatique : MlLib Donn\u00e9es structur\u00e9s et SQL : SparkSQL Flux de donn\u00e9es : Spark streaming et Spark Structured Streaming Graphes : GraphX Architecture syst\u00e8me \u00b6 Le programmeur envoie au framework des Spark Applications, pour lesquelles Spark affecte des ressources (RAM, CPU) du cluster en vue de leur ex\u00e9cution. Un executor n\u2019est responsable que de 2 choses : ex\u00e9cuter le code qui lui est assign\u00e9 par le driver et lui rapporter l\u2019\u00e9tat d\u2019avancement de la t\u00e2che. Le driver est accessible programmatiquement par un point d\u2019entr\u00e9e appel\u00e9 SparkSession, que l\u2019on trouve derri\u00e8re une variable spark. Spark est \u00e9crit en Scala mais les programmes Spark peuvent \u00eatre \u00e9crits en Scala, Java, Python, SQL et R. Dans ce qui suit, nous utilisons Python \u00e0 travers la librairie Pyspark . RDDs, DataFrames et Datasets \u00b6 La principale innovation apport\u00e9e par Spark est le concept de Resilient Distributed Dataset (RDD). Un RDD est une collection calcul\u00e9e \u00e0 partir d\u2019une source de donn\u00e9es (liste, fichier, ...) et plac\u00e9e en m\u00e9moire RAM. Spark conserve l\u2019historique des op\u00e9rations qui a permis de constituer un RDD, et la reprise sur panne s\u2019appuie essentiellement sur la pr\u00e9servation de cet historique afin de reconstituer le RDD en cas de panne. Chaque RDD \u00e9tant fragment\u00e9e, une panne affectant un fragment est r\u00e9par\u00e9e (par reconstitution de l\u2019historique) ind\u00e9pendamment des autres fragments, \u00e9vitant d\u2019avoir \u00e0 tout recalculer. Il est \u00e0 noter que ces RDD sont immuables et ne peuvent \u00eatre modifi\u00e9es apr\u00e8s leur cr\u00e9ation. Les DataFrames et Datasets se situe \u00e0 un niveau plus haut et reposent sur les RDDs. Il apporte une structure aux donn\u00e9es \u00e0 travers un sch\u00e9ma. Pyspark \u00b6 PySpark est une biblioth\u00e8que Spark \u00e9crite en Python pour ex\u00e9cuter une application Python en utilisant les capacit\u00e9s d'Apache Spark, en utilisant PySpark, nous pouvons ex\u00e9cuter des applications en parall\u00e8le sur le cluster distribu\u00e9 (plusieurs n\u0153uds). Les modules et packages de pyspark sont : PySpark RDD (pyspark.RDD) PySpark DataFrame et SQL (pyspark.sql) PySpark Streaming (pyspark.streaming) PySpark MLib (pyspark.ml, pyspark.mllib) PySpark GraphFrames (GraphFrames) PySpark Resource (pyspark.resource) Pr\u00e9paration de l'environnement \u00b6 En utilisant VirtualBox et \u00e0 partir du menu Fichier -> Importer un appareil virtuel, s\u00e9lectionner le fichier BDTools.ova fourni et choisissez l'emplacement de destination. Une fois l'importation finie, v\u00e9rifier les param\u00e8tres de la machine virtuelle pour les ajuster \u00e0 la configuration de votre machine. Il est recommand\u00e9 d'utiliser 8G de RAM en gardant au moins 2G pour la machine h\u00f4te. D\u00e9marrer la machine virtuelle et se connecter avec l'utilisateur uti . Le mot de passe =uti= . Par d\u00e9faut, les services actifs sont HDFS, YARN et HBase. La commande suivante permet de s\u00e9lectionner les services n\u00e9cessaires \u00e0 Spark pour optimiser la gestion de la m\u00e9moire : sudo SelectService spark Cette commande peut prendre quelques minutes selon la configuration de votre machine. Lancer le navigateur et aller sur la page http://localhost o\u00f9 l'\u00e9tat des services est affich\u00e9. Autres Environnements \u00b6 Plusieurs autres alternatives sont disponibles afin de mettre en place un environnement pour reproduire les tests sur Spark. Deux environnements d'ex\u00e9cution sont pr\u00e9sent\u00e9s ci-apr\u00e8s : La machine virtuelle Cloudera QuickStart Un cluster Spark de 3 n\u0153uds avec Hadoop Databricks cloud (version d'essai) Cloudera QuickStart VM \u00b6 Machine Virtuelle Les \u00e9tapes sont simples. Il suffit de : T\u00e9l\u00e9charger l'image de la machine qui convient \u00e0 votre logiciel de virtualisation ( Oracle Virtual Box V6.1 ou Vmware) : Image Cloudera QuickStart 5.13 : pour VirtualBox pour vmware Ensuite, d\u00e9compresser puis importer l'image .ovf \u00e0 partir du logiciel de virtualisation. Enfin, d\u00e9marrer la machine qui est sous Centos 6. Docker Si vous pr\u00e9f\u00e9rez docker alors : Charger l'image docker : Option 1 : docker pull cloudera/quickstart:latest Option 2 : t\u00e9l\u00e9charger l'image docker et la charger avec : docker load < cloudera.tar.gz D\u00e9marrer le container : docker run -m 4G --memory-reservation 2G --memory-swap 8G --hostname = quickstart.cloudera --privileged = true -t -i --publish-all = true -p8888 -p8088 cloudera/quickstart /usr/bin/docker-quickstart Cluster Spark avec Docker \u00b6 Cr\u00e9er un cluster Spark compos\u00e9 de 3 n\u0153uds. T\u00e9l\u00e9charger et d\u00e9compresser l'archive spark-lab.zip spark-lab.zip Cette archieve contient : cluster.sh : script pour la gestion du cluster sous Linux cluster.ps1 : script pour la gestion du cluster pour Windows Powershell spark-apps : dossier contenant l'exemple wordcount. Il est sera mont\u00e9 comme un volume pour docker. Charger l'image docker docker pull hhmida/sparkbase D\u00e9ployer le cluster Shell Linux Windows Powershell ./cluster.sh deploy cluster.ps1 deploy Options du script cluster.sh stop # Arr\u00eater les containers cluster.sh start # D\u00e9marrer les containers cluster.sh info # Montre les URLs pour acc\u00e9der aux services cluster.sh deploy # Formater le cluster et red\u00e9ployer Vu que les ports sont publi\u00e9s sur la machine h\u00f4te, localhost peut \u00eatre utilis\u00e9 avec les URLs des services. Databricks \u00b6","title":"Introduction"},{"location":"index.html#calcul-distribue-avec-spark","text":"","title":"Calcul distribu\u00e9 avec Spark"},{"location":"index.html#objectifs","text":"Utiliser Spark sous Hadoop Cr\u00e9er des programmes avec l'API Spark Manipuler les diff\u00e9rentes structures de donn\u00e9es Spark : RDD, DataFrame Interroger les donn\u00e9es avec SparKSQL Traiter des flux de donn\u00e9es avec Spark Streaming Cr\u00e9er des pipeline ML Outils Image de machine virtuelle avec les outils pr\u00e9-install\u00e9s. C'est une image cr\u00e9\u00e9e par Pierre Nezric \u00e0 laquelle sont ajout\u00e9s jupyter et mrjob . Elle contient les outils suivants : Hadoop 2.7.3 Spark 2.1.1 Pig 0.15.0 Hive 1.2.1 HBase 1.1.9 Cassandra Elasticsearch et Kibana Zookeeper 3.4.6 Sources et r\u00e9f\u00e9rences Documentation Spark","title":"Objectifs"},{"location":"index.html#presentation-de-spark","text":"Avec MapReduce, la sp\u00e9cification de l\u2019it\u00e9ration reste \u00e0 la charge du programmeur; il faut stocker le r\u00e9sultat d\u2019un premier job dans une collection interm\u00e9diaire et r\u00e9iterer le job en prenant la collection interm\u00e9diaire comme source. C\u2019est laborieux pour l\u2019implantation, et surtout tr\u00e8s peu efficace quand la collection interm\u00e9diaire est grande. Le processus de s\u00e9rialisation/d\u00e9s\u00e9rialisation sur disque propre \u00e0 la gestion de la reprise sur panne en MapReduce entra\u00eene des performances m\u00e9diocres. Dans Spark, la m\u00e9thode est tr\u00e8s diff\u00e9rente. Elle consiste \u00e0 placer ces jeux de donn\u00e9es en m\u00e9moire RAM et \u00e0 \u00e9viter la p\u00e9nalit\u00e9 des \u00e9critures sur le disque. Spark offre un \u00e9cosyst\u00e8me de librairies couvrant : L'apprentissage automatique : MlLib Donn\u00e9es structur\u00e9s et SQL : SparkSQL Flux de donn\u00e9es : Spark streaming et Spark Structured Streaming Graphes : GraphX","title":"Pr\u00e9sentation de Spark"},{"location":"index.html#architecture-systeme","text":"Le programmeur envoie au framework des Spark Applications, pour lesquelles Spark affecte des ressources (RAM, CPU) du cluster en vue de leur ex\u00e9cution. Un executor n\u2019est responsable que de 2 choses : ex\u00e9cuter le code qui lui est assign\u00e9 par le driver et lui rapporter l\u2019\u00e9tat d\u2019avancement de la t\u00e2che. Le driver est accessible programmatiquement par un point d\u2019entr\u00e9e appel\u00e9 SparkSession, que l\u2019on trouve derri\u00e8re une variable spark. Spark est \u00e9crit en Scala mais les programmes Spark peuvent \u00eatre \u00e9crits en Scala, Java, Python, SQL et R. Dans ce qui suit, nous utilisons Python \u00e0 travers la librairie Pyspark .","title":"Architecture syst\u00e8me"},{"location":"index.html#rdds-dataframes-et-datasets","text":"La principale innovation apport\u00e9e par Spark est le concept de Resilient Distributed Dataset (RDD). Un RDD est une collection calcul\u00e9e \u00e0 partir d\u2019une source de donn\u00e9es (liste, fichier, ...) et plac\u00e9e en m\u00e9moire RAM. Spark conserve l\u2019historique des op\u00e9rations qui a permis de constituer un RDD, et la reprise sur panne s\u2019appuie essentiellement sur la pr\u00e9servation de cet historique afin de reconstituer le RDD en cas de panne. Chaque RDD \u00e9tant fragment\u00e9e, une panne affectant un fragment est r\u00e9par\u00e9e (par reconstitution de l\u2019historique) ind\u00e9pendamment des autres fragments, \u00e9vitant d\u2019avoir \u00e0 tout recalculer. Il est \u00e0 noter que ces RDD sont immuables et ne peuvent \u00eatre modifi\u00e9es apr\u00e8s leur cr\u00e9ation. Les DataFrames et Datasets se situe \u00e0 un niveau plus haut et reposent sur les RDDs. Il apporte une structure aux donn\u00e9es \u00e0 travers un sch\u00e9ma.","title":"RDDs, DataFrames et Datasets"},{"location":"index.html#pyspark","text":"PySpark est une biblioth\u00e8que Spark \u00e9crite en Python pour ex\u00e9cuter une application Python en utilisant les capacit\u00e9s d'Apache Spark, en utilisant PySpark, nous pouvons ex\u00e9cuter des applications en parall\u00e8le sur le cluster distribu\u00e9 (plusieurs n\u0153uds). Les modules et packages de pyspark sont : PySpark RDD (pyspark.RDD) PySpark DataFrame et SQL (pyspark.sql) PySpark Streaming (pyspark.streaming) PySpark MLib (pyspark.ml, pyspark.mllib) PySpark GraphFrames (GraphFrames) PySpark Resource (pyspark.resource)","title":"Pyspark"},{"location":"index.html#preparation-de-lenvironnement","text":"En utilisant VirtualBox et \u00e0 partir du menu Fichier -> Importer un appareil virtuel, s\u00e9lectionner le fichier BDTools.ova fourni et choisissez l'emplacement de destination. Une fois l'importation finie, v\u00e9rifier les param\u00e8tres de la machine virtuelle pour les ajuster \u00e0 la configuration de votre machine. Il est recommand\u00e9 d'utiliser 8G de RAM en gardant au moins 2G pour la machine h\u00f4te. D\u00e9marrer la machine virtuelle et se connecter avec l'utilisateur uti . Le mot de passe =uti= . Par d\u00e9faut, les services actifs sont HDFS, YARN et HBase. La commande suivante permet de s\u00e9lectionner les services n\u00e9cessaires \u00e0 Spark pour optimiser la gestion de la m\u00e9moire : sudo SelectService spark Cette commande peut prendre quelques minutes selon la configuration de votre machine. Lancer le navigateur et aller sur la page http://localhost o\u00f9 l'\u00e9tat des services est affich\u00e9.","title":"Pr\u00e9paration de l'environnement"},{"location":"index.html#autres-environnements","text":"Plusieurs autres alternatives sont disponibles afin de mettre en place un environnement pour reproduire les tests sur Spark. Deux environnements d'ex\u00e9cution sont pr\u00e9sent\u00e9s ci-apr\u00e8s : La machine virtuelle Cloudera QuickStart Un cluster Spark de 3 n\u0153uds avec Hadoop Databricks cloud (version d'essai)","title":"Autres Environnements"},{"location":"index.html#cloudera-quickstart-vm","text":"Machine Virtuelle Les \u00e9tapes sont simples. Il suffit de : T\u00e9l\u00e9charger l'image de la machine qui convient \u00e0 votre logiciel de virtualisation ( Oracle Virtual Box V6.1 ou Vmware) : Image Cloudera QuickStart 5.13 : pour VirtualBox pour vmware Ensuite, d\u00e9compresser puis importer l'image .ovf \u00e0 partir du logiciel de virtualisation. Enfin, d\u00e9marrer la machine qui est sous Centos 6. Docker Si vous pr\u00e9f\u00e9rez docker alors : Charger l'image docker : Option 1 : docker pull cloudera/quickstart:latest Option 2 : t\u00e9l\u00e9charger l'image docker et la charger avec : docker load < cloudera.tar.gz D\u00e9marrer le container : docker run -m 4G --memory-reservation 2G --memory-swap 8G --hostname = quickstart.cloudera --privileged = true -t -i --publish-all = true -p8888 -p8088 cloudera/quickstart /usr/bin/docker-quickstart","title":"Cloudera QuickStart VM"},{"location":"index.html#cluster-spark-avec-docker","text":"Cr\u00e9er un cluster Spark compos\u00e9 de 3 n\u0153uds. T\u00e9l\u00e9charger et d\u00e9compresser l'archive spark-lab.zip spark-lab.zip Cette archieve contient : cluster.sh : script pour la gestion du cluster sous Linux cluster.ps1 : script pour la gestion du cluster pour Windows Powershell spark-apps : dossier contenant l'exemple wordcount. Il est sera mont\u00e9 comme un volume pour docker. Charger l'image docker docker pull hhmida/sparkbase D\u00e9ployer le cluster Shell Linux Windows Powershell ./cluster.sh deploy cluster.ps1 deploy Options du script cluster.sh stop # Arr\u00eater les containers cluster.sh start # D\u00e9marrer les containers cluster.sh info # Montre les URLs pour acc\u00e9der aux services cluster.sh deploy # Formater le cluster et red\u00e9ployer Vu que les ports sont publi\u00e9s sur la machine h\u00f4te, localhost peut \u00eatre utilis\u00e9 avec les URLs des services.","title":"Cluster Spark avec Docker"},{"location":"index.html#databricks","text":"","title":"Databricks"},{"location":"exercice.html","text":"Exercice \u00b6 Cet exercice porte sur la programmation Spark et SparkSQL. T\u00e9l\u00e9charger les donn\u00e9es sur : Les ventes mensuelles \u00e0 l'export de d\u00e9riv\u00e9s de Phosphate : vente_export_der_ph_mois.csv La production mensuelle de d\u00e9riv\u00e9s de Phosphate : prod_der_phosphate_mois.csv Mettre les deux fichiers csv sur HDFS. En utilisant les RDD de Spark, \u00e9crire un programme Python permettant de r\u00e9pondre aux deux questions suivantes : Quelle est la plus grande production mensuelle de NPK (quantit\u00e9 et date). Les productions annuelles de chaque produit d\u00e9riv\u00e9. Le volume des productions annuelles tous produits confondus. Les ventes annuelles de chaque produit d\u00e9riv\u00e9. La diff\u00e9rence entre (productions annuelles - ventes \u00e0 l'export annuelles) de chaque produit. R\u00e9pondre aux questions pr\u00e9c\u00e9dentes en utilisant les DataFrames. Directives \u00e0 suivre Supprimer l'ent\u00eate du fichier csv avec les RDD (Question 3) : rddP = sc . textFile ( 'hdfs://nodemaster:9000/user/hadoop/laproductionmensuelledesderivesdephosphate.csv' ) . cache () rddP = rddP . mapPartitionsWithIndex ( lambda idx , iter : islice ( iter , 1 , None ) if ( idx == 0 ) else iter ) - Pr\u00e9voir un traitement pour les valeurs manquantes. - Enregistrer chaque r\u00e9ponse individuellement dans HDFS. ( rdd.saveAsTextFile(...) )","title":"Exercice"},{"location":"exercice.html#exercice","text":"Cet exercice porte sur la programmation Spark et SparkSQL. T\u00e9l\u00e9charger les donn\u00e9es sur : Les ventes mensuelles \u00e0 l'export de d\u00e9riv\u00e9s de Phosphate : vente_export_der_ph_mois.csv La production mensuelle de d\u00e9riv\u00e9s de Phosphate : prod_der_phosphate_mois.csv Mettre les deux fichiers csv sur HDFS. En utilisant les RDD de Spark, \u00e9crire un programme Python permettant de r\u00e9pondre aux deux questions suivantes : Quelle est la plus grande production mensuelle de NPK (quantit\u00e9 et date). Les productions annuelles de chaque produit d\u00e9riv\u00e9. Le volume des productions annuelles tous produits confondus. Les ventes annuelles de chaque produit d\u00e9riv\u00e9. La diff\u00e9rence entre (productions annuelles - ventes \u00e0 l'export annuelles) de chaque produit. R\u00e9pondre aux questions pr\u00e9c\u00e9dentes en utilisant les DataFrames. Directives \u00e0 suivre Supprimer l'ent\u00eate du fichier csv avec les RDD (Question 3) : rddP = sc . textFile ( 'hdfs://nodemaster:9000/user/hadoop/laproductionmensuelledesderivesdephosphate.csv' ) . cache () rddP = rddP . mapPartitionsWithIndex ( lambda idx , iter : islice ( iter , 1 , None ) if ( idx == 0 ) else iter ) - Pr\u00e9voir un traitement pour les valeurs manquantes. - Enregistrer chaque r\u00e9ponse individuellement dans HDFS. ( rdd.saveAsTextFile(...) )","title":"Exercice"},{"location":"rdd.html","text":"Manipuler les RDD \u00b6 Les RDD prennent en charge deux types d'op\u00e9rations: les transformations , qui cr\u00e9ent un nouvel RDD \u00e0 partir d'un RDD existant, et les actions , qui renvoient une valeur au programme Driver apr\u00e8s avoir ex\u00e9cut\u00e9 un calcul sur RDD. Toutes les transformations dans Spark sont paresseuses, en ce sens qu'elles ne calculent pas leurs r\u00e9sultats tout de suite. Au lieu de cela, ils se souviennent simplement des transformations appliqu\u00e9es \u00e0 un ensemble de donn\u00e9es de base (par exemple, un fichier). Les transformations ne sont calcul\u00e9es que lorsqu'une action n\u00e9cessite qu'un r\u00e9sultat soit renvoy\u00e9 au programme pilote. Par d\u00e9faut, chaque RDD transform\u00e9 peut \u00eatre recalcul\u00e9 chaque fois que vous ex\u00e9cutez une action dessus. Cependant, vous pouvez \u00e9galement conserver un RDD en m\u00e9moire \u00e0 l'aide de la m\u00e9thode persist (ou cache), auquel cas Spark conservera les \u00e9l\u00e9ments sur le cluster pour un acc\u00e8s beaucoup plus rapide la prochaine fois que vous l'interrogerez. Il existe \u00e9galement une prise en charge des RDD persistants sur disque ou r\u00e9pliqu\u00e9s sur plusieurs n\u0153uds. Transformations \u00b6 .table,th, td{ border-bottom: thin black solid; } th{ font-weight: bold; } Transformation Description map ( func ) Retourne un RDD avec les \u00e9l\u00e9ments obtenus de l'application de func sur les \u00e9l\u00e9ments deu RDD intial. filter ( func ) Construit un RDD avec les \u00e9l\u00e9ments sur lesquels func retourne True. flatMap ( func ) Comme map mais la fonction func retourne une liste d'\u00e9l\u00e9ments associ\u00e9e \u00e0 chaque \u00e9l\u00e9ment du RDD initial. mapPartitions ( func ) Similar to map, but runs separately on each partition (block) of the RDD, so func must be of type Iterator<T> => Iterator<U> when running on an RDD of type T. mapPartitionsWithIndex ( func ) Comme mapPartition et fournit le num\u00e9ro de la partition \u00e0 utiliser. sample ( withReplacement , fraction , seed ) S\u00e9lectionner une fraction des donn\u00e9es, avec ou sans remise. union ( otherDataset ), intersection Op\u00e9rations ensemblistes. distinct ([ numPartitions ])) Retourne un RDD contenant les valeurs uniques. reduceByKey ( func , [ numPartitions ]) Appel\u00e9e sur un RDD de paires (K, V) et retourne aussi un RDD de apires (K, V) o\u00f9 les valeurs sont agr\u00e9g\u00e9es selon la fonction func . Le nombre de t\u00e2ches reduce est d\u00e9termin\u00e9e par le second param\u00e8tre. aggregateByKey ( zeroValue )( seqOp , combOp , [ numPartitions ]) Permet d'obtenir un r\u00e9sultat d'agr\u00e9gation de type diff\u00e9rent de celui des valeurs agr\u00e9g\u00e9es. sortByKey ([ ascending ], [ numPartitions ]) Tri selon la cl\u00e9. join ( otherDataset , [ numPartitions ]) Jointure de deux RDDs de paires (K, V) and (K, W) qui retourne une RDD de paires (K, (V, W)). La jointure externe est r\u00e9alis\u00e9e avec leftOuterJoin , rightOuterJoin , et fullOuterJoin . cartesian ( otherDataset ) Produit cart\u00e9sien de deux RDDs. coalesce ( numPartitions ) Diminuer le nombre de partitions. Utile suite \u00e0 un filtrage du RDD. Actions \u00b6 Action Description reduce ( func ) Applique la fonction d'agr\u00e9gation func . collect () Retourne les \u00e9l\u00e9ments du RDD sous la frorme de liste au Driver. count () Retourne le nombre d'\u00e9l\u00e9ments du RDD. first () Retourne le premier \u00e9l\u00e9ment (\u00e9quivalent \u00e0 take(1)). take ( n ) Retourne un tableau des n premiers \u00e9l\u00e9ments. takeSample ( withReplacement , num , [ seed ]) Combinaison de sample et take . takeOrdered ( n , [ordering] ) Return the first n elements of the RDD using either their natural order or a custom comparator. saveAsTextFile ( path ) Enregistre les \u00e9l\u00e9ments dans un fichier texte (chaque \u00e9l\u00e9ment est converti en cha\u00eene de caract\u00e8res). countByKey () Retourne le nombre d'\u00e9l\u00e9ments pour chaque cl\u00e9. foreach ( func ) Applique une fonction sur chaque \u00e9l\u00e9ment sans retour. Par exemple la sauvegarde dans un fichier. Persistance des RDD \u00b6 C'est la cl\u00e9 pour le gain de performance lors des traitements it\u00e9ratifs sur les RDDs. Il s'agit de garder en m\u00e9moire les donn\u00e9es pour de futures manipulations (voir l'exemple dans la section pr\u00e9c\u00e9dente). Pour activer la persistance sur un RDD, il suffit d'appleer persist() ou cache() . Il est possible de d\u00e9finir des niveaux de persistance avec persist(StorageLevel.<niveau>) ou niveau peut \u00eatre : MEMORY_ONLY : En m\u00e9moire. Si la m\u00e9moire est insuffisante les partitions non charg\u00e9es en m\u00e9moire seront calcul\u00e9es \u00e0 la vol\u00e9e au besoin. C'est le niveau par d\u00e9faut (utilis\u00e9 avec cache()). MEMORY_AND_DISK Les partitions qui d\u00e9apssent la capacit\u00e9 de la m\u00e9moire sont stock\u00e9es sur disque. MEMORY_ONLY_SER et MEMORY_AND_DISK_SER (Java and Scala) : idem. mais les donn\u00e9es sont s\u00e9rialis\u00e9s. Avec Python, les RDD sont toujours s\u00e9rialis\u00e9es au format pickle. DISK_ONLY : stockage sur disque seulement. MEMORY_ONLY_2 , MEMORY_AND_DISK_2 , etc. : avec r\u00e9plication sur 2 noeuds. Exemples \u00b6 Ouvrir dans un nouvel onglet T\u00e9l\u00e9charger le notebook DemoRDD.ipynb","title":"Manipuler les RDD"},{"location":"rdd.html#manipuler-les-rdd","text":"Les RDD prennent en charge deux types d'op\u00e9rations: les transformations , qui cr\u00e9ent un nouvel RDD \u00e0 partir d'un RDD existant, et les actions , qui renvoient une valeur au programme Driver apr\u00e8s avoir ex\u00e9cut\u00e9 un calcul sur RDD. Toutes les transformations dans Spark sont paresseuses, en ce sens qu'elles ne calculent pas leurs r\u00e9sultats tout de suite. Au lieu de cela, ils se souviennent simplement des transformations appliqu\u00e9es \u00e0 un ensemble de donn\u00e9es de base (par exemple, un fichier). Les transformations ne sont calcul\u00e9es que lorsqu'une action n\u00e9cessite qu'un r\u00e9sultat soit renvoy\u00e9 au programme pilote. Par d\u00e9faut, chaque RDD transform\u00e9 peut \u00eatre recalcul\u00e9 chaque fois que vous ex\u00e9cutez une action dessus. Cependant, vous pouvez \u00e9galement conserver un RDD en m\u00e9moire \u00e0 l'aide de la m\u00e9thode persist (ou cache), auquel cas Spark conservera les \u00e9l\u00e9ments sur le cluster pour un acc\u00e8s beaucoup plus rapide la prochaine fois que vous l'interrogerez. Il existe \u00e9galement une prise en charge des RDD persistants sur disque ou r\u00e9pliqu\u00e9s sur plusieurs n\u0153uds.","title":"Manipuler les RDD"},{"location":"rdd.html#transformations","text":".table,th, td{ border-bottom: thin black solid; } th{ font-weight: bold; } Transformation Description map ( func ) Retourne un RDD avec les \u00e9l\u00e9ments obtenus de l'application de func sur les \u00e9l\u00e9ments deu RDD intial. filter ( func ) Construit un RDD avec les \u00e9l\u00e9ments sur lesquels func retourne True. flatMap ( func ) Comme map mais la fonction func retourne une liste d'\u00e9l\u00e9ments associ\u00e9e \u00e0 chaque \u00e9l\u00e9ment du RDD initial. mapPartitions ( func ) Similar to map, but runs separately on each partition (block) of the RDD, so func must be of type Iterator<T> => Iterator<U> when running on an RDD of type T. mapPartitionsWithIndex ( func ) Comme mapPartition et fournit le num\u00e9ro de la partition \u00e0 utiliser. sample ( withReplacement , fraction , seed ) S\u00e9lectionner une fraction des donn\u00e9es, avec ou sans remise. union ( otherDataset ), intersection Op\u00e9rations ensemblistes. distinct ([ numPartitions ])) Retourne un RDD contenant les valeurs uniques. reduceByKey ( func , [ numPartitions ]) Appel\u00e9e sur un RDD de paires (K, V) et retourne aussi un RDD de apires (K, V) o\u00f9 les valeurs sont agr\u00e9g\u00e9es selon la fonction func . Le nombre de t\u00e2ches reduce est d\u00e9termin\u00e9e par le second param\u00e8tre. aggregateByKey ( zeroValue )( seqOp , combOp , [ numPartitions ]) Permet d'obtenir un r\u00e9sultat d'agr\u00e9gation de type diff\u00e9rent de celui des valeurs agr\u00e9g\u00e9es. sortByKey ([ ascending ], [ numPartitions ]) Tri selon la cl\u00e9. join ( otherDataset , [ numPartitions ]) Jointure de deux RDDs de paires (K, V) and (K, W) qui retourne une RDD de paires (K, (V, W)). La jointure externe est r\u00e9alis\u00e9e avec leftOuterJoin , rightOuterJoin , et fullOuterJoin . cartesian ( otherDataset ) Produit cart\u00e9sien de deux RDDs. coalesce ( numPartitions ) Diminuer le nombre de partitions. Utile suite \u00e0 un filtrage du RDD.","title":"Transformations"},{"location":"rdd.html#actions","text":"Action Description reduce ( func ) Applique la fonction d'agr\u00e9gation func . collect () Retourne les \u00e9l\u00e9ments du RDD sous la frorme de liste au Driver. count () Retourne le nombre d'\u00e9l\u00e9ments du RDD. first () Retourne le premier \u00e9l\u00e9ment (\u00e9quivalent \u00e0 take(1)). take ( n ) Retourne un tableau des n premiers \u00e9l\u00e9ments. takeSample ( withReplacement , num , [ seed ]) Combinaison de sample et take . takeOrdered ( n , [ordering] ) Return the first n elements of the RDD using either their natural order or a custom comparator. saveAsTextFile ( path ) Enregistre les \u00e9l\u00e9ments dans un fichier texte (chaque \u00e9l\u00e9ment est converti en cha\u00eene de caract\u00e8res). countByKey () Retourne le nombre d'\u00e9l\u00e9ments pour chaque cl\u00e9. foreach ( func ) Applique une fonction sur chaque \u00e9l\u00e9ment sans retour. Par exemple la sauvegarde dans un fichier.","title":"Actions"},{"location":"rdd.html#persistance-des-rdd","text":"C'est la cl\u00e9 pour le gain de performance lors des traitements it\u00e9ratifs sur les RDDs. Il s'agit de garder en m\u00e9moire les donn\u00e9es pour de futures manipulations (voir l'exemple dans la section pr\u00e9c\u00e9dente). Pour activer la persistance sur un RDD, il suffit d'appleer persist() ou cache() . Il est possible de d\u00e9finir des niveaux de persistance avec persist(StorageLevel.<niveau>) ou niveau peut \u00eatre : MEMORY_ONLY : En m\u00e9moire. Si la m\u00e9moire est insuffisante les partitions non charg\u00e9es en m\u00e9moire seront calcul\u00e9es \u00e0 la vol\u00e9e au besoin. C'est le niveau par d\u00e9faut (utilis\u00e9 avec cache()). MEMORY_AND_DISK Les partitions qui d\u00e9apssent la capacit\u00e9 de la m\u00e9moire sont stock\u00e9es sur disque. MEMORY_ONLY_SER et MEMORY_AND_DISK_SER (Java and Scala) : idem. mais les donn\u00e9es sont s\u00e9rialis\u00e9s. Avec Python, les RDD sont toujours s\u00e9rialis\u00e9es au format pickle. DISK_ONLY : stockage sur disque seulement. MEMORY_ONLY_2 , MEMORY_AND_DISK_2 , etc. : avec r\u00e9plication sur 2 noeuds.","title":"Persistance des RDD"},{"location":"rdd.html#exemples","text":"Ouvrir dans un nouvel onglet T\u00e9l\u00e9charger le notebook DemoRDD.ipynb","title":"Exemples"},{"location":"sparksql.html","text":"SparkSQL \u00b6 SparkSQL est un module Spark qui se base sur les RDD permettant de les d\u00f4ter d'une structure ou sch\u00e9ma et les interroger \u00e0 travers le SQL en utilisant les DataFrames. Contexte SQL \u00b6 L'cc\u00e8s aux objets et m\u00e9thodes de l'API SparkSQL est possible \u00e0 partir d'un objet SparkSession ou SQLContext. Contexte SQL SparkSession SQLContext from pyspark.sql import SparkSession spark = SparkSession . builder . appName ( 'Exemple SparkSQL' ) . getOrCreate () from pyspark import SparkConf , SparkContext from pyspark.sql import SQLContext sc = SparkContext ( conf = SparkConf () . setAppName ( 'Exemple SparkSQL' )) sqlc = SQLContext ( sc ) Cr\u00e9ation de DataFrame \u00b6 Elle peut \u00eatre r\u00e9alis\u00e9e de 2 fa\u00e7ons : \u00e0 partir de RDDs existantes ou DataFrame pandas : spark.createDataFrame(data, schema=None, samplingRatio=None, verifySchema=True) ou rdd.toDF(schema=None, samplingRatio=None) \u00e0 partir de sources externes (json, csv, ...) : spark.read.<format> (semblables aux m\u00e9thodes de la biblioth\u00e8que pandas) rdd = spark . sparkContext . parallelize ([( 'Stylo' , 0.625 , 10 ),( 'Rame papier' , 9.200 , 5 )]) df = spark . createDataFrame ( rdd , schema = [ 'Produit' , 'Prix' , 'Qte' ]) Commandes SQL \u00b6 Afin de pouvoir utiliser le DataFrame dans les requ\u00eates SQL, il faut la d\u00e9clarer comme une vue temporaire. Cr\u00e9ation de vue temporaire df . createOrReplaceTempView ( \"table1\" ) Par la suite, lancer une requ\u00eate SQL est fait avec la m\u00e9thode spark.sql : Cr\u00e9ation de vue temporaire spark . sql ( \"SELECT * FROM table1\" ) API DataFrame \u00b6 Elle comporte une liste assez riche d'op\u00e9rations. Parmi lesquelles, nous pouvons citer : Op\u00e9rations de type SQL : df.select(col1, col2, ...) df.where(condition) df.groupBy(col1, col2, ...) df.orderBy(cols, ascending, ...) df.join(other, on=None, how=None) df.limit(num) df.agg(...) Autres op\u00e9rations : df.show(n) df.printSchema() df.describe(col1, col2, ...) df.drop(col1, col2, ...) df.dropna(how='any', thresh=None, subset=None) df.withColumn(colName, col) Exemples \u00b6 Ouvrir dans un nouvel onglet T\u00e9l\u00e9charger le notebook DemoSparkSQL.ipynb","title":"SparkSQL"},{"location":"sparksql.html#sparksql","text":"SparkSQL est un module Spark qui se base sur les RDD permettant de les d\u00f4ter d'une structure ou sch\u00e9ma et les interroger \u00e0 travers le SQL en utilisant les DataFrames.","title":"SparkSQL"},{"location":"sparksql.html#contexte-sql","text":"L'cc\u00e8s aux objets et m\u00e9thodes de l'API SparkSQL est possible \u00e0 partir d'un objet SparkSession ou SQLContext. Contexte SQL SparkSession SQLContext from pyspark.sql import SparkSession spark = SparkSession . builder . appName ( 'Exemple SparkSQL' ) . getOrCreate () from pyspark import SparkConf , SparkContext from pyspark.sql import SQLContext sc = SparkContext ( conf = SparkConf () . setAppName ( 'Exemple SparkSQL' )) sqlc = SQLContext ( sc )","title":"Contexte SQL"},{"location":"sparksql.html#creation-de-dataframe","text":"Elle peut \u00eatre r\u00e9alis\u00e9e de 2 fa\u00e7ons : \u00e0 partir de RDDs existantes ou DataFrame pandas : spark.createDataFrame(data, schema=None, samplingRatio=None, verifySchema=True) ou rdd.toDF(schema=None, samplingRatio=None) \u00e0 partir de sources externes (json, csv, ...) : spark.read.<format> (semblables aux m\u00e9thodes de la biblioth\u00e8que pandas) rdd = spark . sparkContext . parallelize ([( 'Stylo' , 0.625 , 10 ),( 'Rame papier' , 9.200 , 5 )]) df = spark . createDataFrame ( rdd , schema = [ 'Produit' , 'Prix' , 'Qte' ])","title":"Cr\u00e9ation de DataFrame"},{"location":"sparksql.html#commandes-sql","text":"Afin de pouvoir utiliser le DataFrame dans les requ\u00eates SQL, il faut la d\u00e9clarer comme une vue temporaire. Cr\u00e9ation de vue temporaire df . createOrReplaceTempView ( \"table1\" ) Par la suite, lancer une requ\u00eate SQL est fait avec la m\u00e9thode spark.sql : Cr\u00e9ation de vue temporaire spark . sql ( \"SELECT * FROM table1\" )","title":"Commandes SQL"},{"location":"sparksql.html#api-dataframe","text":"Elle comporte une liste assez riche d'op\u00e9rations. Parmi lesquelles, nous pouvons citer : Op\u00e9rations de type SQL : df.select(col1, col2, ...) df.where(condition) df.groupBy(col1, col2, ...) df.orderBy(cols, ascending, ...) df.join(other, on=None, how=None) df.limit(num) df.agg(...) Autres op\u00e9rations : df.show(n) df.printSchema() df.describe(col1, col2, ...) df.drop(col1, col2, ...) df.dropna(how='any', thresh=None, subset=None) df.withColumn(colName, col)","title":"API DataFrame"},{"location":"sparksql.html#exemples","text":"Ouvrir dans un nouvel onglet T\u00e9l\u00e9charger le notebook DemoSparkSQL.ipynb","title":"Exemples"},{"location":"streaming.html","text":"Spark Streaming \u00b6 Spark offre la possibilit\u00e9 de traiter des donn\u00e9es en flux \u00e0 partir de plusieurs sources (kafka, twitter, TCP, ...). Spark transforme le flux continue de donn\u00e9es en batchs \u00e0 des intervalles de temps r\u00e9guliers. Il cr\u00e9e ainsi une s\u00e9quence continue de RDDs de m\u00eame type accessibles \u00e0 travers la classe DStream . Cette classe cr\u00e9e un RDD \u00e0 des intervalles de temps r\u00e9guliers. Le point d'entr\u00e9e aux fonctionnalit\u00e9s de Streaming de Spark est le StreamingContext . Il permet de cr\u00e9er des instances de DStream . StreamingContext \u00b6 Cr\u00e9ation \u00b6 StreamingContext ( sparkContext , batchDuration = None ) Exemple from pyspark.sql import SparkSession from pyspark.streaming import StreamingContext spark = SparkSession . builder . appName ( 'Exemple Streaming' ) . getOrCreate () ssc = StreamingContext ( spark . sparkContext , 10 ) Principales M\u00e9thodes \u00b6 socketTextStream(hostname, port) : flux de donn\u00e9es \u00e0 partir de socket TCP (codage UTF8 et \\n pour fin de ligne). textFileStream(directory) : surveille un dossier et lit les nouveaux fichiers dans le format texte. binaryRecordsStream(directory, recordLength) : comme textFileStream mais les fichiers sont lus dans le format binaire avec des enregistrements de taille fixe = recordLength . queueStream(rdds, oneAtATime=True, default=None) : une file de RDDs comme source de donn\u00e9es. start() : d\u00e9marre le traitement des flux. stop(stopSparkContext=True, stopGraceFully=False) : arr\u00eat du traitement des flux. awaitTermination(timeout=None) : attente de la fin d'ex\u00e9cution. DStream (Discretized Stream) \u00b6 Une instance DStream est caract\u00e9ris\u00e9e par : La liste d'autres DStreams sur lesquelles elle d\u00e9pend. Un intervalle au bout duquel elle g\u00e9n\u00e8re un RDD. Une fonction utilis\u00e9e pour la g\u00e9n\u00e9ration du RDD. DStream poss\u00e8de plusieurs m\u00e9thodes communes avec RDD comme : `map(), flatMap(), reduce(), reduceByKey(), filter(), count(), join(), ... Elles g\u00e9n\u00e9rent \u00e9galement des DStreams. Autres m\u00e9thodes : pprint([num]) : affiche les n premiers \u00e9l\u00e9ments. window(windowDuration[, slideDuration]) : cr\u00e9e un nouveau DStream o\u00f9 chaque RDD contient les \u00e9l\u00e9ments re\u00e7us dans une fen\u00eatre de temps sur le DStream. Exemples \u00b6 Dans ce qui suit nous montrons comment traiter un flux continu de messages en provenance de Kafka. Install and Setup Kafka Cluster Spark Streaming avec socketTextStream \u00b6 D\u00e9marrer netcat nc -l -p 9999 D\u00e9marrer l'application Spark spark-submit network_wordcount.py localhost 9999 network_wordcount.py import sys from pyspark import SparkContext from pyspark.streaming import StreamingContext if __name__ == \"__main__\" : if len ( sys . argv ) != 3 : print ( \"Usage: network_wordcount.py <hostname> <port>\" , file = sys . stderr ) sys . exit ( - 1 ) sc = SparkContext ( appName = \"PythonStreamingNetworkWordCount\" ) ssc = StreamingContext ( sc , 5 ) lines = ssc . socketTextStream ( sys . argv [ 1 ], int ( sys . argv [ 2 ])) counts = lines . flatMap ( lambda line : line . split ( \" \" )) \\ . map ( lambda word : ( word , 1 )) \\ . reduceByKey ( lambda a , b : a + b ) counts . pprint () ssc . start () ssc . awaitTermination () Spark Streaming avec Kafka \u00b6 Installation de Kafka \u00b6 Si kafka n'est pas install\u00e9 alors suivre les \u00e9tapes suivantes : T\u00e9l\u00e9charger les binaires de Kafka wget https://archive.apache.org/dist/kafka/0.10.2.2/kafka_2.12-0.10.2.2.tgz D\u00e9compresser et renommer le dossier obtenu tar -xzf kafka_2.12-0.10.2.2.tgz mv kafka_2.12-0.10.2.2.tgz kafka Ajouter l'emplacement de Kafka \u00e0 la variable PATH dans .bashrc ou .profile echo \"PATH= $PATH :~/kafka/bin\" >> ~/.bashrc source ~/.bashrc D\u00e9marrer l'exemple \u00b6 Les commandes sont ex\u00e9cut\u00e9es \u00e0 partir du dossier kafka D\u00e9marrer Zookeeper bin/zookeeper-server-start.sh config/zookeeper.properties Zookeeper ZooKeeper est un service de coordination pour les applications distribu\u00e9es.ZooKeeper enregistrer les m\u00e9tadonn\u00e9es concernant le cluster (h\u00f4tes, ports, ...). Il est fourni avec Kafka. D\u00e9marrer le broker Kafka dans une nouvelle session bin/kafka-server-start.sh config/server.properties Cr\u00e9er un topic sur Kafka Dans une nouvelle session : bin/kafka-topics.sh --create --zookeeper localhost:2181 \\ --replication-factor 1 \\ --partitions 1 \\ --topic wordcounttopic Topic Kafka Les messages Kafka sont organis\u00e9s en topics. Ces derniers sont partitionn\u00e9s et r\u00e9pliqu\u00e9es sur de multiples brokers \u00e0 travers le cluster. Un processus Producer envoie les messages au topic et un processus consumer les lit. Dans cet exemple, le producer est le programme fourni par Kafka, le consumer est Spark. Pour voir les topics disponibles : bin/kafka-topics.sh --zookeeper localhost:2181 --list D\u00e9marrer le producer dans une nouvelle session bin/kafka-console-producer.sh --broker-list localhost:9092 --topic wordcounttopic D\u00e9marrer l'application Spark spark-submit --master yarn --deploy-mode client --conf \"spark.dynamicAllocation.enabled=false\" \\ --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.0 kafka_wordcount.py \\ localhost:2181 wordcounttopic kafka_wordcount.py from __future__ import print_function import sys from pyspark import SparkContext from pyspark.streaming import StreamingContext from pyspark.streaming.kafka import KafkaUtils if __name__ == \"__main__\" : if len ( sys . argv ) != 3 : print ( \"Usage: kafka_wordcount.py <zk> <topic>\" , file = sys . stderr ) sys . exit ( - 1 ) sc = SparkContext ( appName = \"PythonStreamingKafkaWordCount\" ) ssc = StreamingContext ( sc , 10 ) zkQuorum , topic = sys . argv [ 1 :] kvs = KafkaUtils . createStream ( ssc , zkQuorum , \"spark-streamingconsumer\" , { topic : 1 }) lines = kvs . map ( lambda x : x [ 1 ]) counts = lines . flatMap ( lambda line : line . split ( \" \" )) \\ . map ( lambda word :( word , 1 )) . reduceByKey ( lambda a , b : a + b ) counts . pprint () ssc . start () ssc . awaitTermination () Taper du texte dans la session du producer et inspecter l'affichage obtenu par Messages de log Si Spark affiche trop de messages d'information, vous pouvez r\u00e9duire le niveau de journalisation en modifiant la ligne du fichier spark/conf/log4j.properties : log4j.rootCategory = ERROR, console Structured Streaming \u00b6 Spark Structured Streaming offre un moteur de traitement de flux au-dessus de Spark SQL. Il permet d'exprimer les traitements sur les flux de la m\u00eame fa\u00e7on que sur des donn\u00e9es statiques. Spark SQL se charge de l'ex\u00e9cution continue et incr\u00e9mentale et la mise \u00e0 jour du r\u00e9sultat final. Spark r\u00e9alise ce traitement avec une latence de l'ordre de 1 milliseconde. Les flux de donn\u00e9es sont repr\u00e9sent\u00e9es sous forme de DataFrame qui peut \u00eatre assimil\u00e9e \u00e0 une table de taille illimit\u00e9e dont le contenu augmente au cours du temps avec la r\u00e9ception du flux. L'API Spark, repose sur la classe DataStreamReder pour charger un DataFrame de flux \u00e0 partir d'une source cpmme HDFS, Kafka, Kinesis, ...). Le traitement sur ce flux est exprim\u00e9 via une requ\u00eate StreamingQuery . L'\u00e9criture d'un flux est prise en charge par la classe DataStreamWriter obtenu \u00e0 partir de DataFrame.writeStream . Le mod\u00e8le de programmation est repr\u00e9sent\u00e9 dans la figure suivante : L'instance DataStremReader est obtenu \u00e0 partir de SparkSession par : readStream . Elle poss\u00e8de les m\u00e9thodes : format(source) : sp\u00e9cifie la source (socket, kafka, ...) option(key, value) : ajoute une configuration pour la source csv(path[, schema, sep, encoding, quote, \u2026]) : charge un fichier csv text(path[, wholetext, lineSep, \u2026]) : charge un fichier texte load([path, format, schema]) : charge le flux de la source et le retourne sous la forme de DataFrame Et voici quelques m\u00e9thode la classe DataStreamWriter : format(source) : sp\u00e9cifie la source (socket, kafka, ...) option(key, value) : ajoute une configuration pour la source start([path, format, outputMode, \u2026]) : charge le flux de la source et le retourne sous la forme de DataFrame outputMode(outputMode) : append, complete, ... awaitTermination([timeout]) stop() processAllAvailable() Exemple \u00b6 Suivre les m\u00eames \u00e9tapes qua dans l'exemple socketTextStream . structured_network_wordcount.py import sys , os , json # Spark from pyspark.sql import SparkSession # Spark Streaming from pyspark.streaming import StreamingContext from pyspark.sql.functions import explode from pyspark.sql.functions import split spark = SparkSession . builder . appName ( 'Spark Structured Streaming' ) . getOrCreate () # Create SPARK Context sc = spark . sparkContext sc . setLogLevel ( \"WARN\" ) ## Create Streaming context , with 10 second interval ssc = StreamingContext ( sc , 10 ) # Create DataFrame representing the stream of input lines from connection to localhost:9999 lines = spark \\ . readStream \\ . format ( \"socket\" ) \\ . option ( \"host\" , \"localhost\" ) \\ . option ( \"port\" , 9999 ) \\ . load () # Split the lines into words words = lines . select ( explode ( split ( lines . value , \" \" ) ) . alias ( \"word\" ) ) # Generate running word count wordCounts = words . groupBy ( \"word\" ) . count () # Start running the query that prints the running counts to the console query = wordCounts \\ . writeStream \\ . outputMode ( \"complete\" ) \\ . format ( \"console\" ) \\ . start () query . awaitTermination () spark-submit structured_network_wordcount.py","title":"Spark Streaming"},{"location":"streaming.html#spark-streaming","text":"Spark offre la possibilit\u00e9 de traiter des donn\u00e9es en flux \u00e0 partir de plusieurs sources (kafka, twitter, TCP, ...). Spark transforme le flux continue de donn\u00e9es en batchs \u00e0 des intervalles de temps r\u00e9guliers. Il cr\u00e9e ainsi une s\u00e9quence continue de RDDs de m\u00eame type accessibles \u00e0 travers la classe DStream . Cette classe cr\u00e9e un RDD \u00e0 des intervalles de temps r\u00e9guliers. Le point d'entr\u00e9e aux fonctionnalit\u00e9s de Streaming de Spark est le StreamingContext . Il permet de cr\u00e9er des instances de DStream .","title":"Spark Streaming"},{"location":"streaming.html#streamingcontext","text":"","title":"StreamingContext"},{"location":"streaming.html#creation","text":"StreamingContext ( sparkContext , batchDuration = None ) Exemple from pyspark.sql import SparkSession from pyspark.streaming import StreamingContext spark = SparkSession . builder . appName ( 'Exemple Streaming' ) . getOrCreate () ssc = StreamingContext ( spark . sparkContext , 10 )","title":"Cr\u00e9ation"},{"location":"streaming.html#principales-methodes","text":"socketTextStream(hostname, port) : flux de donn\u00e9es \u00e0 partir de socket TCP (codage UTF8 et \\n pour fin de ligne). textFileStream(directory) : surveille un dossier et lit les nouveaux fichiers dans le format texte. binaryRecordsStream(directory, recordLength) : comme textFileStream mais les fichiers sont lus dans le format binaire avec des enregistrements de taille fixe = recordLength . queueStream(rdds, oneAtATime=True, default=None) : une file de RDDs comme source de donn\u00e9es. start() : d\u00e9marre le traitement des flux. stop(stopSparkContext=True, stopGraceFully=False) : arr\u00eat du traitement des flux. awaitTermination(timeout=None) : attente de la fin d'ex\u00e9cution.","title":"Principales M\u00e9thodes"},{"location":"streaming.html#dstream-discretized-stream","text":"Une instance DStream est caract\u00e9ris\u00e9e par : La liste d'autres DStreams sur lesquelles elle d\u00e9pend. Un intervalle au bout duquel elle g\u00e9n\u00e8re un RDD. Une fonction utilis\u00e9e pour la g\u00e9n\u00e9ration du RDD. DStream poss\u00e8de plusieurs m\u00e9thodes communes avec RDD comme : `map(), flatMap(), reduce(), reduceByKey(), filter(), count(), join(), ... Elles g\u00e9n\u00e9rent \u00e9galement des DStreams. Autres m\u00e9thodes : pprint([num]) : affiche les n premiers \u00e9l\u00e9ments. window(windowDuration[, slideDuration]) : cr\u00e9e un nouveau DStream o\u00f9 chaque RDD contient les \u00e9l\u00e9ments re\u00e7us dans une fen\u00eatre de temps sur le DStream.","title":"DStream (Discretized Stream)"},{"location":"streaming.html#exemples","text":"Dans ce qui suit nous montrons comment traiter un flux continu de messages en provenance de Kafka. Install and Setup Kafka Cluster","title":"Exemples"},{"location":"streaming.html#spark-streaming-avec-sockettextstream","text":"D\u00e9marrer netcat nc -l -p 9999 D\u00e9marrer l'application Spark spark-submit network_wordcount.py localhost 9999 network_wordcount.py import sys from pyspark import SparkContext from pyspark.streaming import StreamingContext if __name__ == \"__main__\" : if len ( sys . argv ) != 3 : print ( \"Usage: network_wordcount.py <hostname> <port>\" , file = sys . stderr ) sys . exit ( - 1 ) sc = SparkContext ( appName = \"PythonStreamingNetworkWordCount\" ) ssc = StreamingContext ( sc , 5 ) lines = ssc . socketTextStream ( sys . argv [ 1 ], int ( sys . argv [ 2 ])) counts = lines . flatMap ( lambda line : line . split ( \" \" )) \\ . map ( lambda word : ( word , 1 )) \\ . reduceByKey ( lambda a , b : a + b ) counts . pprint () ssc . start () ssc . awaitTermination ()","title":"Spark Streaming avec socketTextStream"},{"location":"streaming.html#spark-streaming-avec-kafka","text":"","title":"Spark Streaming avec Kafka"},{"location":"streaming.html#installation-de-kafka","text":"Si kafka n'est pas install\u00e9 alors suivre les \u00e9tapes suivantes : T\u00e9l\u00e9charger les binaires de Kafka wget https://archive.apache.org/dist/kafka/0.10.2.2/kafka_2.12-0.10.2.2.tgz D\u00e9compresser et renommer le dossier obtenu tar -xzf kafka_2.12-0.10.2.2.tgz mv kafka_2.12-0.10.2.2.tgz kafka Ajouter l'emplacement de Kafka \u00e0 la variable PATH dans .bashrc ou .profile echo \"PATH= $PATH :~/kafka/bin\" >> ~/.bashrc source ~/.bashrc","title":"Installation de Kafka"},{"location":"streaming.html#demarrer-lexemple","text":"Les commandes sont ex\u00e9cut\u00e9es \u00e0 partir du dossier kafka D\u00e9marrer Zookeeper bin/zookeeper-server-start.sh config/zookeeper.properties Zookeeper ZooKeeper est un service de coordination pour les applications distribu\u00e9es.ZooKeeper enregistrer les m\u00e9tadonn\u00e9es concernant le cluster (h\u00f4tes, ports, ...). Il est fourni avec Kafka. D\u00e9marrer le broker Kafka dans une nouvelle session bin/kafka-server-start.sh config/server.properties Cr\u00e9er un topic sur Kafka Dans une nouvelle session : bin/kafka-topics.sh --create --zookeeper localhost:2181 \\ --replication-factor 1 \\ --partitions 1 \\ --topic wordcounttopic Topic Kafka Les messages Kafka sont organis\u00e9s en topics. Ces derniers sont partitionn\u00e9s et r\u00e9pliqu\u00e9es sur de multiples brokers \u00e0 travers le cluster. Un processus Producer envoie les messages au topic et un processus consumer les lit. Dans cet exemple, le producer est le programme fourni par Kafka, le consumer est Spark. Pour voir les topics disponibles : bin/kafka-topics.sh --zookeeper localhost:2181 --list D\u00e9marrer le producer dans une nouvelle session bin/kafka-console-producer.sh --broker-list localhost:9092 --topic wordcounttopic D\u00e9marrer l'application Spark spark-submit --master yarn --deploy-mode client --conf \"spark.dynamicAllocation.enabled=false\" \\ --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.0 kafka_wordcount.py \\ localhost:2181 wordcounttopic kafka_wordcount.py from __future__ import print_function import sys from pyspark import SparkContext from pyspark.streaming import StreamingContext from pyspark.streaming.kafka import KafkaUtils if __name__ == \"__main__\" : if len ( sys . argv ) != 3 : print ( \"Usage: kafka_wordcount.py <zk> <topic>\" , file = sys . stderr ) sys . exit ( - 1 ) sc = SparkContext ( appName = \"PythonStreamingKafkaWordCount\" ) ssc = StreamingContext ( sc , 10 ) zkQuorum , topic = sys . argv [ 1 :] kvs = KafkaUtils . createStream ( ssc , zkQuorum , \"spark-streamingconsumer\" , { topic : 1 }) lines = kvs . map ( lambda x : x [ 1 ]) counts = lines . flatMap ( lambda line : line . split ( \" \" )) \\ . map ( lambda word :( word , 1 )) . reduceByKey ( lambda a , b : a + b ) counts . pprint () ssc . start () ssc . awaitTermination () Taper du texte dans la session du producer et inspecter l'affichage obtenu par Messages de log Si Spark affiche trop de messages d'information, vous pouvez r\u00e9duire le niveau de journalisation en modifiant la ligne du fichier spark/conf/log4j.properties : log4j.rootCategory = ERROR, console","title":"D\u00e9marrer l'exemple"},{"location":"streaming.html#structured-streaming","text":"Spark Structured Streaming offre un moteur de traitement de flux au-dessus de Spark SQL. Il permet d'exprimer les traitements sur les flux de la m\u00eame fa\u00e7on que sur des donn\u00e9es statiques. Spark SQL se charge de l'ex\u00e9cution continue et incr\u00e9mentale et la mise \u00e0 jour du r\u00e9sultat final. Spark r\u00e9alise ce traitement avec une latence de l'ordre de 1 milliseconde. Les flux de donn\u00e9es sont repr\u00e9sent\u00e9es sous forme de DataFrame qui peut \u00eatre assimil\u00e9e \u00e0 une table de taille illimit\u00e9e dont le contenu augmente au cours du temps avec la r\u00e9ception du flux. L'API Spark, repose sur la classe DataStreamReder pour charger un DataFrame de flux \u00e0 partir d'une source cpmme HDFS, Kafka, Kinesis, ...). Le traitement sur ce flux est exprim\u00e9 via une requ\u00eate StreamingQuery . L'\u00e9criture d'un flux est prise en charge par la classe DataStreamWriter obtenu \u00e0 partir de DataFrame.writeStream . Le mod\u00e8le de programmation est repr\u00e9sent\u00e9 dans la figure suivante : L'instance DataStremReader est obtenu \u00e0 partir de SparkSession par : readStream . Elle poss\u00e8de les m\u00e9thodes : format(source) : sp\u00e9cifie la source (socket, kafka, ...) option(key, value) : ajoute une configuration pour la source csv(path[, schema, sep, encoding, quote, \u2026]) : charge un fichier csv text(path[, wholetext, lineSep, \u2026]) : charge un fichier texte load([path, format, schema]) : charge le flux de la source et le retourne sous la forme de DataFrame Et voici quelques m\u00e9thode la classe DataStreamWriter : format(source) : sp\u00e9cifie la source (socket, kafka, ...) option(key, value) : ajoute une configuration pour la source start([path, format, outputMode, \u2026]) : charge le flux de la source et le retourne sous la forme de DataFrame outputMode(outputMode) : append, complete, ... awaitTermination([timeout]) stop() processAllAvailable()","title":"Structured Streaming"},{"location":"streaming.html#exemple","text":"Suivre les m\u00eames \u00e9tapes qua dans l'exemple socketTextStream . structured_network_wordcount.py import sys , os , json # Spark from pyspark.sql import SparkSession # Spark Streaming from pyspark.streaming import StreamingContext from pyspark.sql.functions import explode from pyspark.sql.functions import split spark = SparkSession . builder . appName ( 'Spark Structured Streaming' ) . getOrCreate () # Create SPARK Context sc = spark . sparkContext sc . setLogLevel ( \"WARN\" ) ## Create Streaming context , with 10 second interval ssc = StreamingContext ( sc , 10 ) # Create DataFrame representing the stream of input lines from connection to localhost:9999 lines = spark \\ . readStream \\ . format ( \"socket\" ) \\ . option ( \"host\" , \"localhost\" ) \\ . option ( \"port\" , 9999 ) \\ . load () # Split the lines into words words = lines . select ( explode ( split ( lines . value , \" \" ) ) . alias ( \"word\" ) ) # Generate running word count wordCounts = words . groupBy ( \"word\" ) . count () # Start running the query that prints the running counts to the console query = wordCounts \\ . writeStream \\ . outputMode ( \"complete\" ) \\ . format ( \"console\" ) \\ . start () query . awaitTermination () spark-submit structured_network_wordcount.py","title":"Exemple"},{"location":"submitting.html","text":"Cr\u00e9er des applications Spark \u00b6 Une application Spark est coordonn\u00e9e par l'objet SparkContext cr\u00e9\u00e9 dans le programme principal appel\u00e9 Driver . Ce dernier se connecte \u00e0 un gestionnaire de cluster (Spark, Yarn, Mesos, Kubernetes) qui lui allouera les ressources n\u00e9cessaires. Le Driver obtient alors les Executors (processus sur les n\u0153uds du cluster) et par la suite leur envoie le code puis les t\u00e2ches \u00e0 r\u00e9aliser. (voir figure ci-apr\u00e8s) Le lancement d'une application Spark peut \u00eatre fait selon 2 m\u00e9thodes : Une ex\u00e9cution interactive Soumission de jobs \u00e0 un gestionnaire : Spark Standalone, YARN, Meos ou Kubernetes Ex\u00e9cution interactive \u00b6 C'est une ex\u00e9cution sur le n\u0153ud local dont l'utilit\u00e9 est de tester l'application Spark. Avec le shell pyspark \u00b6 Spark dispose d'un shell pour interpr\u00e9ter les instruction Python qui peut \u00eatre invoqu\u00e9 avec la commande pyspark . Un objet SparContext est alors cr\u00e9\u00e9 et plac\u00e9 dans la variable sc . Le programme est ex\u00e9cut\u00e9 en simulant un nombre de n\u0153uds \u00e9gal au nombre de cores logiques du processeur (voir Local[*] plus bas). Pour voir l'\u00e9tat du job, aller sur : http://localhost:4040 . Cette interface n'est disponible que pendant pyspark est en cours d'ex\u00e9cution. Exemple Wordcount \u00b6 Mettre le fichier shakespeare.txt sous HDFS. hadoop fs -put shakespeare.txt Lancer pyspark pyspark Ex\u00e9cuter les instructions suivantes une \u00e0 une : texte = sc . textFile ( 'shakespeare.txt' ) count = texte . flatMap ( lambda l : l . split ()) . map ( lambda w :( w , 1 )) . reduceByKey ( lambda a , b : a + b ) count . collect () Notebook jupyter \u00b6 T\u00e9l\u00e9charger le notebook wordcount.ipynb Lancer Jupyter depuis le dossier contenant wordcount.ipynb jupyter notebook Acc\u00e9der au notebook jupyter wordcount.ipynb Spark UI Surveiller le job sur http://localhost:4040 remarquer que l'ex\u00e9cution ne se d\u00e9clenche r\u00e9ellement qu'apr\u00e8s l'appel de la m\u00e9thode collect() . Parmi les onglets de Spark UI : Jobs : Un job est l\u2019ex\u00e9cution d\u2019une cha\u00eene de traitements (workflow) dans un environnement distribu\u00e9. Le workflow peut \u00eatre visualis\u00e9 \u00e0 partir de DAG Visualization . Le nombre de jobs est \u00e9agal au nombre d'actions sur les RDDs. Dans cet exemple, l\u2019ex\u00e9cution s\u2019est faite en deux \u00e9tapes (Stage). La premi\u00e8re comprend les transformations textFile, flatMap et map, la seconde la transformation reduceByKey. les deux \u00e9tapes sont s\u00e9par\u00e9es par une phase de shuffle. L\u2019ex\u00e9cution d\u2019une \u00e9tape se fait par un ensemble de t\u00e2ches, une par machine h\u00e9bergeant un fragment du RDD. Stages : Il montre de nombreuses statistiques sur le temps d\u2019ex\u00e9cution, le volume des donn\u00e9es \u00e9chang\u00e9es, ... de chaque \u00e9tape. Storage : Contient les donn\u00e9es en m\u00e9moire pendant l'ex\u00e9cution. On peut demander explicitement de garder en m\u00e9moire un RDD avec cache() ou persist() . persist() Inspecter l'onglet Storage apr\u00e8s l'ex\u00e9cution de la cellule avec le code count.persist() puis comparer les temps d'ex\u00e9cution des deux instructions collect() . Google Colab Pour ex\u00e9cuter le Notebook pr\u00e9c\u00e9dent en utilisant Google Colab, suivre les \u00e9tapes suivantes : Acc\u00e9der \u00e0 Google Colab https://colab.research.google.com/ Importer le Notebook wordcount.ipynb \u00e0 partir du menu Fichier puis Importer le notebook Ex\u00e9cuter en premier lieu la cellule contenant !pip install pyspark (\u00e0 ins\u00e9rer si elle n'est pas existante) Importer le fichier shakespeare.txt en ex\u00e9cutant !wget https://raw.githubusercontent.com/hhmida/tp-spark/main/assets/shakespeare.txt Ex\u00e9cuter les diff\u00e9rentes cellules Soumission avec spark-submit \u00b6 C'est un script Spark pour soumettre des jobs dans les langages accept\u00e9s sans interaction. Avec les param\u00e8tres il est possible de choisir le mode d'ex\u00e9cution, la r\u00e9servation des ressources, la configuration Spark... La diff\u00e9rence, par rapport \u00e0 pyspark, est qu'il faut cr\u00e9er une instance SparkContext dans le programme Python soumis : sc = SparkContext ( SparkConf ( ... )) spark-submit spark-submit \\ --master <master-url> \\ --deploy-mode <client | cluster> \\ --conf <key< = <value> \\ --driver-memory <value>g \\ --executor-memory <value>g \\ --executor-cores <number of cores> \\ --jars <comma separated dependencies> \\ --py-files <comma separated dependencies> \\ --class <main-class> \\ <application-jar-py> \\ [ application-arguments ] En local \u00b6 Mode similaire \u00e0 pyspark sans l'interactivit\u00e9. Mode local Commande spark-submit --master local [ * ] wordcount.py UI http://localhost:4040 Spark standalone cluster \u00b6 Mode Standalone Cluster Commande spark-submit --master spark://localhost:7077 UI http://localhost:8888 yarn \u00b6 Mode Yarn Commande spark-submit --master yarn --deploy-mode cluster wordcount.py UI La surveillance est prise en charge avec Hadoop. http://localhost:8080","title":"Cr\u00e9er des applications Spark"},{"location":"submitting.html#creer-des-applications-spark","text":"Une application Spark est coordonn\u00e9e par l'objet SparkContext cr\u00e9\u00e9 dans le programme principal appel\u00e9 Driver . Ce dernier se connecte \u00e0 un gestionnaire de cluster (Spark, Yarn, Mesos, Kubernetes) qui lui allouera les ressources n\u00e9cessaires. Le Driver obtient alors les Executors (processus sur les n\u0153uds du cluster) et par la suite leur envoie le code puis les t\u00e2ches \u00e0 r\u00e9aliser. (voir figure ci-apr\u00e8s) Le lancement d'une application Spark peut \u00eatre fait selon 2 m\u00e9thodes : Une ex\u00e9cution interactive Soumission de jobs \u00e0 un gestionnaire : Spark Standalone, YARN, Meos ou Kubernetes","title":"Cr\u00e9er des applications Spark"},{"location":"submitting.html#execution-interactive","text":"C'est une ex\u00e9cution sur le n\u0153ud local dont l'utilit\u00e9 est de tester l'application Spark.","title":"Ex\u00e9cution interactive"},{"location":"submitting.html#avec-le-shell-pyspark","text":"Spark dispose d'un shell pour interpr\u00e9ter les instruction Python qui peut \u00eatre invoqu\u00e9 avec la commande pyspark . Un objet SparContext est alors cr\u00e9\u00e9 et plac\u00e9 dans la variable sc . Le programme est ex\u00e9cut\u00e9 en simulant un nombre de n\u0153uds \u00e9gal au nombre de cores logiques du processeur (voir Local[*] plus bas). Pour voir l'\u00e9tat du job, aller sur : http://localhost:4040 . Cette interface n'est disponible que pendant pyspark est en cours d'ex\u00e9cution.","title":"Avec le shell pyspark"},{"location":"submitting.html#exemple-wordcount","text":"Mettre le fichier shakespeare.txt sous HDFS. hadoop fs -put shakespeare.txt Lancer pyspark pyspark Ex\u00e9cuter les instructions suivantes une \u00e0 une : texte = sc . textFile ( 'shakespeare.txt' ) count = texte . flatMap ( lambda l : l . split ()) . map ( lambda w :( w , 1 )) . reduceByKey ( lambda a , b : a + b ) count . collect ()","title":"Exemple Wordcount"},{"location":"submitting.html#notebook-jupyter","text":"T\u00e9l\u00e9charger le notebook wordcount.ipynb Lancer Jupyter depuis le dossier contenant wordcount.ipynb jupyter notebook Acc\u00e9der au notebook jupyter wordcount.ipynb Spark UI Surveiller le job sur http://localhost:4040 remarquer que l'ex\u00e9cution ne se d\u00e9clenche r\u00e9ellement qu'apr\u00e8s l'appel de la m\u00e9thode collect() . Parmi les onglets de Spark UI : Jobs : Un job est l\u2019ex\u00e9cution d\u2019une cha\u00eene de traitements (workflow) dans un environnement distribu\u00e9. Le workflow peut \u00eatre visualis\u00e9 \u00e0 partir de DAG Visualization . Le nombre de jobs est \u00e9agal au nombre d'actions sur les RDDs. Dans cet exemple, l\u2019ex\u00e9cution s\u2019est faite en deux \u00e9tapes (Stage). La premi\u00e8re comprend les transformations textFile, flatMap et map, la seconde la transformation reduceByKey. les deux \u00e9tapes sont s\u00e9par\u00e9es par une phase de shuffle. L\u2019ex\u00e9cution d\u2019une \u00e9tape se fait par un ensemble de t\u00e2ches, une par machine h\u00e9bergeant un fragment du RDD. Stages : Il montre de nombreuses statistiques sur le temps d\u2019ex\u00e9cution, le volume des donn\u00e9es \u00e9chang\u00e9es, ... de chaque \u00e9tape. Storage : Contient les donn\u00e9es en m\u00e9moire pendant l'ex\u00e9cution. On peut demander explicitement de garder en m\u00e9moire un RDD avec cache() ou persist() . persist() Inspecter l'onglet Storage apr\u00e8s l'ex\u00e9cution de la cellule avec le code count.persist() puis comparer les temps d'ex\u00e9cution des deux instructions collect() . Google Colab Pour ex\u00e9cuter le Notebook pr\u00e9c\u00e9dent en utilisant Google Colab, suivre les \u00e9tapes suivantes : Acc\u00e9der \u00e0 Google Colab https://colab.research.google.com/ Importer le Notebook wordcount.ipynb \u00e0 partir du menu Fichier puis Importer le notebook Ex\u00e9cuter en premier lieu la cellule contenant !pip install pyspark (\u00e0 ins\u00e9rer si elle n'est pas existante) Importer le fichier shakespeare.txt en ex\u00e9cutant !wget https://raw.githubusercontent.com/hhmida/tp-spark/main/assets/shakespeare.txt Ex\u00e9cuter les diff\u00e9rentes cellules","title":"Notebook jupyter"},{"location":"submitting.html#soumission-avec-spark-submit","text":"C'est un script Spark pour soumettre des jobs dans les langages accept\u00e9s sans interaction. Avec les param\u00e8tres il est possible de choisir le mode d'ex\u00e9cution, la r\u00e9servation des ressources, la configuration Spark... La diff\u00e9rence, par rapport \u00e0 pyspark, est qu'il faut cr\u00e9er une instance SparkContext dans le programme Python soumis : sc = SparkContext ( SparkConf ( ... )) spark-submit spark-submit \\ --master <master-url> \\ --deploy-mode <client | cluster> \\ --conf <key< = <value> \\ --driver-memory <value>g \\ --executor-memory <value>g \\ --executor-cores <number of cores> \\ --jars <comma separated dependencies> \\ --py-files <comma separated dependencies> \\ --class <main-class> \\ <application-jar-py> \\ [ application-arguments ]","title":"Soumission avec spark-submit"},{"location":"submitting.html#en-local","text":"Mode similaire \u00e0 pyspark sans l'interactivit\u00e9. Mode local Commande spark-submit --master local [ * ] wordcount.py UI http://localhost:4040","title":"En local"},{"location":"submitting.html#spark-standalone-cluster","text":"Mode Standalone Cluster Commande spark-submit --master spark://localhost:7077 UI http://localhost:8888","title":"Spark standalone cluster"},{"location":"submitting.html#yarn","text":"Mode Yarn Commande spark-submit --master yarn --deploy-mode cluster wordcount.py UI La surveillance est prise en charge avec Hadoop. http://localhost:8080","title":"yarn"}]}